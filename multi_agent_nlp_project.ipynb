{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. ç¯å¢ƒä¸ä¾èµ–æ£€æŸ¥\n",
    "\n",
    "å¦‚æœä½ è¿˜æ²¡æœ‰æŒ‰ç…§ `requirements.txt` å®‰è£…ä¾èµ–ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢çš„å‘½ä»¤ï¼ˆå¯æŒ‰éœ€æ‰§è¡Œä¸€æ¬¡ï¼‰ï¼š\n",
    "\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "åœ¨ Notebook ä¸­ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼š\n",
    "\n",
    "```python\n",
    "# %pip install -r requirements.txt\n",
    "```\n"
   ],
   "id": "91c173531089b1b5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\", \"https://api.chatanywhere.tech/v1\")\n",
    "LLM_MODEL = os.getenv(\"LLM_MODEL\", \"gpt-4o-mini\")\n",
    "\n",
    "print(\"ğŸ“¦ ç¯å¢ƒå˜é‡æ£€æŸ¥ï¼š\")\n",
    "print(\"  OPENAI_API_KEY:\", \"å·²é…ç½®\" if OPENAI_API_KEY else \"æœªé…ç½® (å°†ä½¿ç”¨ DummyLLM)\")\n",
    "print(\"  SERPAPI_API_KEY:\", \"å·²é…ç½®\" if SERPAPI_API_KEY else \"æœªé…ç½® (æœç´¢å·¥å…·å°†ä¸ºå ä½å®ç°)\")\n",
    "print(\"  OPENAI_BASE_URL:\", OPENAI_BASE_URL)\n",
    "print(\"  LLM_MODEL:\", LLM_MODEL)\n",
    "\n",
    "IS_DEEPSEEK = \"deepseek.com\" in (OPENAI_BASE_URL or \"\").lower()\n",
    "if IS_DEEPSEEK:\n",
    "    lm_lower = (LLM_MODEL or \"\").lower()\n",
    "    if lm_lower not in (\"deepseek-chat\", \"deepseek-reasoner\"):\n",
    "        new_model = \"deepseek-reasoner\" if (\"reason\" in lm_lower or \"think\" in lm_lower) else \"deepseek-chat\"\n",
    "        print(f\"â„¹ï¸ æ£€æµ‹åˆ° DeepSeek æ¥å£ï¼Œè‡ªåŠ¨å°†æ¨¡å‹å '{LLM_MODEL}' è§„èŒƒä¸º '{new_model}'ã€‚\")\n",
    "        LLM_MODEL = new_model\n"
   ],
   "id": "e4fa6a8f57404c22"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2. LLM åˆå§‹åŒ–ï¼ˆä¸è„šæœ¬ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼‰\n",
    "\n",
    "è¿™é‡Œå¤ç”¨è„šæœ¬ä¸­çš„ä¸‰å±‚å›é€€ç­–ç•¥ï¼š\n",
    "1. é¦–é€‰ `langchain_openai.ChatOpenAI`ã€‚\n",
    "2. å¤±è´¥æ—¶é€€åˆ°ç®€æ˜“ HTTP å®¢æˆ·ç«¯ `HTTPFallbackChat`ï¼ˆç›´æ¥è°ƒç”¨ OpenAI å…¼å®¹æ¥å£ï¼‰ã€‚\n",
    "3. å†å¤±è´¥ä½¿ç”¨ `DummyLLM`ï¼Œåªè¾“å‡ºå ä½æ–‡æœ¬ï¼Œæ–¹ä¾¿åœ¨æ—  Key æƒ…å†µä¸‹è°ƒè¯•æµç¨‹ã€‚\n"
   ],
   "id": "f0eac8582e3bf3d8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "class DummyLLM:\n",
    "    \"\"\"ç¼ºå¤± API Key æ—¶ä½¿ç”¨çš„å ä½ LLMï¼Œå®ç° .invoke æ¥å£ã€‚\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model_name = \"dummy-llm\"\n",
    "    def invoke(self, prompt):\n",
    "        if isinstance(prompt, dict):\n",
    "            return f\"[DummyLLM response for keys: {list(prompt.keys())}]\"\n",
    "        return \"[DummyLLM generic response]\"\n",
    "    def __or__(self, other):  # å…¼å®¹é“¾å¼è°ƒç”¨\n",
    "        return other\n",
    "\n",
    "\n",
    "class HTTPFallbackChat:\n",
    "    \"\"\"ç›´æ¥è°ƒç”¨ OpenAI å…¼å®¹æ¥å£çš„è½»é‡å®¢æˆ·ç«¯ï¼Œå®ç° .invoke(dict|str)ã€‚\"\"\"\n",
    "    def __init__(self, base_url: str, api_key: str, model: str, timeout: float = 30.0):\n",
    "        self.base_url = base_url.rstrip('/')\n",
    "        self.api_key = api_key\n",
    "        self.model = model\n",
    "        self.timeout = timeout\n",
    "        if self.base_url.endswith('/v1'):\n",
    "            self.endpoint = f\"{self.base_url}/chat/completions\"\n",
    "        else:\n",
    "            self.endpoint = f\"{self.base_url}/v1/chat/completions\"\n",
    "\n",
    "    def invoke(self, prompt):\n",
    "        if isinstance(prompt, dict):\n",
    "            user_content = '\\n'.join(f\"{k}: {v}\" for k, v in prompt.items())\n",
    "        else:\n",
    "            user_content = str(prompt)\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are an academic writing optimization assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_content}\n",
    "            ],\n",
    "            \"temperature\": 0\n",
    "        }\n",
    "        headers = {\n",
    "            \"Authorization\": f\"Bearer {self.api_key}\",\n",
    "            \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "        try:\n",
    "            resp = requests.post(self.endpoint, headers=headers, json=payload, timeout=self.timeout)\n",
    "            if resp.status_code != 200:\n",
    "                return f\"[HTTPFallbackChat Error {resp.status_code}: {resp.text[:200]}]\"\n",
    "            data = resp.json()\n",
    "            return data.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"[No content]\")\n",
    "        except Exception as e:\n",
    "            return f\"[HTTPFallbackChat Exception: {e}]\"\n",
    "\n",
    "    def __or__(self, other):\n",
    "        return other\n",
    "\n",
    "\n",
    "def init_llm():\n",
    "    if not OPENAI_API_KEY:\n",
    "        print(\"âš ï¸ OPENAI_API_KEY æœªé…ç½®ï¼Œå°†ä½¿ç”¨ DummyLLMï¼ˆä»…ç”¨äºæµç¨‹è°ƒè¯•ï¼‰ã€‚\")\n",
    "        return DummyLLM()\n",
    "    try:\n",
    "        from langchain_openai import ChatOpenAI\n",
    "        llm = ChatOpenAI(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=0,\n",
    "            api_key=(lambda: OPENAI_API_KEY),\n",
    "            base_url=OPENAI_BASE_URL,\n",
    "        )\n",
    "        print(f\"âœ… ä¸» LLM å·²é€šè¿‡ ChatOpenAI åˆå§‹åŒ–: {LLM_MODEL} @ {OPENAI_BASE_URL}\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ChatOpenAI åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°è¯• HTTPFallbackChat...\")\n",
    "        try:\n",
    "            fallback_llm = HTTPFallbackChat(OPENAI_BASE_URL, OPENAI_API_KEY, LLM_MODEL)\n",
    "            probe = fallback_llm.invoke(\"probe\")\n",
    "            if probe.startswith(\"[HTTPFallbackChat Error\") or probe.startswith(\"[HTTPFallbackChat Exception\"):\n",
    "                print(\"âš ï¸ HTTPFallbackChat æ¢æµ‹å¤±è´¥ï¼Œå›é€€åˆ° DummyLLMã€‚\")\n",
    "                return DummyLLM()\n",
    "            print(f\"âœ… HTTPFallbackChat å·²å°±ç»ª: {LLM_MODEL} @ {OPENAI_BASE_URL}\")\n",
    "            return fallback_llm\n",
    "        except Exception as e2:\n",
    "            print(f\"âŒ HTTPFallbackChat åˆå§‹åŒ–å¤±è´¥: {e2}ï¼Œå›é€€åˆ° DummyLLMã€‚\")\n",
    "            return DummyLLM()\n",
    "\n",
    "\n",
    "llm = init_llm()\n"
   ],
   "id": "42e52e7bf7b366db"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. å·¥å…·å±‚ï¼šæœç´¢ / Python REPL / æ–‡ä»¶è¯»å†™\n",
    "\n",
    "è¿™é‡Œä¸è„šæœ¬ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼š\n",
    "- å¦‚æœæ²¡æœ‰ SerpAPI Keyï¼Œåˆ™æœç´¢å·¥å…·è¿”å›å ä½æç¤ºï¼Œä½†æ¥å£ä»ç„¶å¯ç”¨ã€‚\n",
    "- æä¾› Python REPL å·¥å…·æ‰§è¡Œç®€å•ä»£ç ç‰‡æ®µã€‚\n",
    "- æä¾›æœ€å°çš„æ–‡æœ¬æ–‡ä»¶è¯»å†™å·¥å…·ï¼Œæ–¹ä¾¿åœ¨ Notebook ä¸­å¿«é€Ÿè¯•éªŒã€‚\n"
   ],
   "id": "7af93a6aba9840d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.tools import Tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain_community.utilities import SerpAPIWrapper\n",
    "\n",
    "TOOLS = []\n",
    "\n",
    "if SERPAPI_API_KEY:\n",
    "    search_wrapper = SerpAPIWrapper(search_engine=\"google\", serpapi_api_key=SERPAPI_API_KEY)\n",
    "    search_tool = Tool(\n",
    "        name=\"ç½‘ç»œæœç´¢\",\n",
    "        func=search_wrapper.run,\n",
    "        description=\"å®æ—¶ä¿¡æ¯æŸ¥è¯¢ï¼šè¾“å…¥æœç´¢å…³é”®è¯\"\n",
    "    )\n",
    "    TOOLS.append(search_tool)\n",
    "else:\n",
    "    def _search_stub(q: str) -> str:\n",
    "        return f\"[SerpAPI æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæœç´¢: {q}]\"\n",
    "    search_tool = Tool(name=\"ç½‘ç»œæœç´¢\", func=_search_stub, description=\"SerpAPI æœªé…ç½®ï¼Œå ä½æœç´¢å·¥å…·\")\n",
    "    TOOLS.append(search_tool)\n",
    "\n",
    "python_repl = PythonREPL()\n",
    "python_repl_tool = Tool(\n",
    "    name=\"Python REPL\",\n",
    "    func=python_repl.run,\n",
    "    description=\"æ‰§è¡Œæ ¼å¼æ­£ç¡®çš„ Python ä»£ç \"\n",
    ")\n",
    "\n",
    "read_file_tool = Tool(\n",
    "    name=\"è¯»å–æ–‡ä»¶\",\n",
    "    func=lambda fn: open(fn, \"r\", encoding=\"utf-8\").read(),\n",
    "    description=\"è¯»å–æŒ‡å®šæ–‡æœ¬æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ï¼Œè¾“å…¥ä¸ºæ–‡ä»¶è·¯å¾„\"\n",
    ")\n",
    "\n",
    "write_file_tool = Tool(\n",
    "    name=\"å†™å…¥æ–‡ä»¶\",\n",
    "    func=lambda arg: (\n",
    "        (lambda filename, content: (open(filename, \"w\", encoding=\"utf-8\").write(content), \"å†™å…¥å®Œæˆ\")[1])\n",
    "    )(*arg.split(\",\", 1)),\n",
    "    description=\"å†™å…¥æ–‡ä»¶å†…å®¹ã€‚è¾“å…¥æ ¼å¼: æ–‡ä»¶å,å†…å®¹ï¼ˆè‹±æ–‡é€—å·åˆ†éš”ï¼‰\"\n",
    ")\n",
    "\n",
    "TOOLS.extend([python_repl_tool, read_file_tool, write_file_tool])\n",
    "\n",
    "print(f\"ğŸ”§ å·²åŠ è½½å·¥å…·æ•°é‡: {len(TOOLS)}\")\n"
   ],
   "id": "b4b2f4bb465addf0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. å‘é‡è®°å¿†å±‚ï¼šFAISS æˆ–ç®€æ˜“å†…å­˜æ£€ç´¢\n",
    "\n",
    "ä¸è„šæœ¬ç‰ˆæœ¬ä¿æŒä¸€è‡´ï¼š\n",
    "- ä¼˜å…ˆä½¿ç”¨ FAISS + OpenAIEmbeddingsï¼ˆæˆ– DummyEmbeddingsï¼‰ã€‚\n",
    "- å¦‚æœ FAISS/ä¾èµ–ç¼ºå¤±ï¼Œåˆ™é€€å›åˆ°ä¸€ä¸ªåŸºäº token é‡å åº¦çš„ç®€æ˜“ `SimpleVectorStore`ã€‚\n"
   ],
   "id": "950013b5905bbac9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from typing import List, Dict, Optional, Tuple\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "EMBED_DIM = 1536\n",
    "USE_FAISS = True\n",
    "try:\n",
    "    from langchain_community.vectorstores import FAISS\n",
    "    from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "    import faiss\n",
    "    from langchain_core.documents import Document\n",
    "    from langchain_core.embeddings import Embeddings as LCEmbeddings\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ å‘é‡å­˜å‚¨ä¾èµ–ç¼ºå¤±: {e}ï¼Œä½¿ç”¨ç®€æ˜“å†…å­˜æ£€ç´¢æ›¿ä»£ FAISSã€‚\")\n",
    "    USE_FAISS = False\n",
    "    FAISS = None  # type: ignore\n",
    "    InMemoryDocstore = None  # type: ignore\n",
    "    faiss = None  # type: ignore\n",
    "    try:\n",
    "        from langchain_core.documents import Document\n",
    "    except Exception:\n",
    "        class Document:  # æœ€å°å ä½å®ç°\n",
    "            def __init__(self, page_content: str, metadata: Optional[Dict] = None):\n",
    "                self.page_content = page_content\n",
    "                self.metadata = metadata or {}\n",
    "\n",
    "    class LCEmbeddings:  # type: ignore\n",
    "        _dim = EMBED_DIM\n",
    "        def embed_query(self, x: str):\n",
    "            return [0.0] * self._dim\n",
    "        def embed_documents(self, xs: List[str]):\n",
    "            return [[0.0] * self._dim for _ in xs]\n",
    "\n",
    "\n",
    "EMBED_MODEL_NAME = os.getenv(\"EMBED_MODEL_NAME\", \"text-embedding-3-small\")\n",
    "\n",
    "class DummyEmbeddings:\n",
    "    def embed_query(self, t: str):\n",
    "        return [0.0] * EMBED_DIM\n",
    "    def embed_documents(self, docs: List[str]):\n",
    "        return [[0.0] * EMBED_DIM for _ in docs]\n",
    "    def __call__(self, t: str):\n",
    "        return self.embed_query(t)\n",
    "\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    try:\n",
    "        if IS_DEEPSEEK:\n",
    "            print(\"â„¹ï¸ æ£€æµ‹åˆ° DeepSeek base_urlï¼Œè·³è¿‡ OpenAIEmbeddingsï¼Œä½¿ç”¨å ä½ embeddingã€‚\")\n",
    "            embeddings_model = DummyEmbeddings()\n",
    "        else:\n",
    "            from langchain_openai import OpenAIEmbeddings\n",
    "            embeddings_model = OpenAIEmbeddings(\n",
    "                model=EMBED_MODEL_NAME,\n",
    "                api_key=(lambda: OPENAI_API_KEY),\n",
    "                base_url=OPENAI_BASE_URL\n",
    "            )\n",
    "            print(f\"âœ… Embeddings æ¨¡å‹å·²åˆå§‹åŒ–: {EMBED_MODEL_NAME}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Embeddings åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨ DummyEmbeddingsã€‚\")\n",
    "        embeddings_model = DummyEmbeddings()\n",
    "else:\n",
    "    embeddings_model = DummyEmbeddings()\n",
    "    print(\"âš ï¸ OPENAI_API_KEY ç¼ºå¤±ï¼ŒEmbeddings ä½¿ç”¨å ä½å‘é‡ã€‚\")\n",
    "\n",
    "_DEF_WORD_RE = re.compile(r\"[\\u4e00-\\u9fff]|[A-Za-z0-9_]+\")\n",
    "\n",
    "def _simple_tokenize(text: str) -> List[str]:\n",
    "    return _DEF_WORD_RE.findall(text or \"\")\n",
    "\n",
    "\n",
    "if USE_FAISS:\n",
    "    class EmbeddingAdapter(LCEmbeddings):\n",
    "        def __init__(self, base):\n",
    "            self.base = base\n",
    "        def embed_query(self, x: str) -> List[float]:\n",
    "            try:\n",
    "                return self.base.embed_query(x)\n",
    "            except Exception:\n",
    "                return [0.0] * EMBED_DIM\n",
    "        def embed_documents(self, xs: List[str]) -> List[List[float]]:\n",
    "            try:\n",
    "                return self.base.embed_documents(xs)\n",
    "            except Exception:\n",
    "                return [[0.0] * EMBED_DIM for _ in xs]\n",
    "\n",
    "    adapter = EmbeddingAdapter(embeddings_model)\n",
    "    index = faiss.IndexFlatL2(EMBED_DIM)\n",
    "    vectorstore = FAISS(adapter, index, InMemoryDocstore({}), {})\n",
    "    print(\"ğŸ§  å‘é‡æ•°æ®åº“(FAISS)å·²åˆå§‹åŒ–ã€‚\")\n",
    "else:\n",
    "    class SimpleVectorStore:\n",
    "        def __init__(self):\n",
    "            self.docs: List[Document] = []\n",
    "        def add_documents(self, docs: List[Document]):\n",
    "            self.docs.extend(docs)\n",
    "        def similarity_search(self, query: str, k: int = 3) -> List[Document]:\n",
    "            q_tokens = set(_simple_tokenize(query))\n",
    "            def score(doc: Document):\n",
    "                d_tokens = set(_simple_tokenize(doc.page_content))\n",
    "                if not q_tokens or not d_tokens:\n",
    "                    return 0.0\n",
    "                return len(q_tokens & d_tokens) / len(q_tokens | d_tokens)\n",
    "            ranked = sorted(self.docs, key=score, reverse=True)\n",
    "            return ranked[:k]\n",
    "\n",
    "    vectorstore = SimpleVectorStore()\n",
    "    print(\"ğŸ§  å‘é‡æ•°æ®åº“ç®€åŒ–ç‰ˆå·²åˆå§‹åŒ–ï¼ˆæ—  FAISSï¼‰ã€‚\")\n",
    "\n",
    "\n",
    "class MemoryManager:\n",
    "    def __init__(self, vs, namespace: str = \"global\"):\n",
    "        self.vs = vs\n",
    "        self.namespace = namespace\n",
    "        self._counter = 0\n",
    "    def add_memory(self, text: str, metadata: Optional[Dict] = None):\n",
    "        try:\n",
    "            meta = metadata or {}\n",
    "            meta.update({\"namespace\": self.namespace, \"ts\": datetime.now().isoformat()})\n",
    "            doc = Document(page_content=text, metadata=meta)\n",
    "            self.vs.add_documents([doc])\n",
    "            self._counter += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ å†™å…¥è®°å¿†å¤±è´¥: {e}\")\n",
    "    def recall(self, query: str, k: int = 3) -> List[str]:\n",
    "        try:\n",
    "            res = self.vs.similarity_search(query, k=k)\n",
    "            return [d.page_content for d in res]\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¯»å–è®°å¿†å¤±è´¥: {e}\")\n",
    "            return []\n"
   ],
   "id": "1fa71de0a79adc45"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. åŒ Agent å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–ç³»ç»Ÿï¼ˆä¸è„šæœ¬ç‰ˆæœ¬åŒæ­¥ï¼‰\n",
    "\n",
    "è¿™é‡Œç›´æ¥å¤ç”¨ `multi_agent_nlp_project.py` ä¸­çš„å®ç°ï¼Œåªä¿ç•™ä¸â€œåä½œä¼˜åŒ–â€å¼ºç›¸å…³çš„éƒ¨åˆ†ï¼Œæ–¹ä¾¿åœ¨ Notebook ä¸­äº¤äº’å¼è°ƒç”¨ã€‚\n"
   ],
   "id": "f6ccef539b72e4d2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import difflib\n",
    "import time\n",
    "from html import escape as _html_escape  # æ–°å¢ï¼šHTML æŠ¥å‘Šæ‰€éœ€\n",
    "try:\n",
    "    from metrics import AcademicMetrics\n",
    "except Exception:\n",
    "    AcademicMetrics = None\n",
    "\n",
    "# ä¿®æ”¹ä¸æ‰©å±• DualAgentAcademicSystemï¼šåŠ å…¥åŒ LLMã€ä»£ç å—æ‰§è¡Œã€æ•°æ®åˆæˆ/è¯„ä¼°/è’¸é¦ã€HTML æŠ¥å‘Šã€é•¿æ–‡æœ¬ä¼˜åŒ–ç­‰\n",
    "class DualAgentAcademicSystem:\n",
    "    def __init__(self, llm, tools, vectorstore, enable_tools: bool = True, enable_memory: bool = True,\n",
    "                 agent_a_llm=None, agent_b_llm=None):\n",
    "        \"\"\"åŒ Agent å­¦æœ¯ç³»ç»Ÿï¼ˆNotebook åŒæ­¥ç‰ˆï¼‰\n",
    "\n",
    "        å‚æ•°:\n",
    "          llm: é»˜è®¤ LLMï¼Œå½“æœªæä¾› agent_a_llm / agent_b_llm æ—¶ä½¿ç”¨ã€‚\n",
    "          tools: LangChain Tool åˆ—è¡¨ã€‚\n",
    "          vectorstore: å‘é‡å­˜å‚¨æˆ–ç®€æ˜“ SimpleVectorStoreã€‚\n",
    "          enable_tools: æ˜¯å¦å¯ç”¨å¤–éƒ¨å·¥å…·è°ƒç”¨ã€‚\n",
    "          enable_memory: æ˜¯å¦å¯ç”¨é•¿ç¨‹è®°å¿†ã€‚\n",
    "          agent_a_llm: ï¼ˆå¯é€‰ï¼‰Agent A ä¸“ç”¨ LLMï¼ˆä¼˜åŒ–è€…ï¼Œå¦‚æœ¬åœ°å­¦ç”Ÿæ¨¡å‹ï¼‰ã€‚\n",
    "          agent_b_llm: ï¼ˆå¯é€‰ï¼‰Agent B ä¸“ç”¨ LLMï¼ˆè¯„å®¡è€…ï¼Œå¦‚è¿œç¨‹æ•™å¸ˆæ¨¡å‹ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        self.llm = llm\n",
    "        self.agent_a_llm = agent_a_llm or llm\n",
    "        self.agent_b_llm = agent_b_llm or llm\n",
    "        self.tools_enabled = enable_tools\n",
    "        self.memory_enabled = enable_memory\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.vectorstore = vectorstore\n",
    "        self.memory = MemoryManager(vectorstore) if enable_memory else None\n",
    "        self.collaboration_log: List[Dict] = []\n",
    "        self._setup_agents()\n",
    "\n",
    "    def _setup_agents(self):\n",
    "        self.agent_a_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "ä½ æ˜¯Agent A - å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–ä¸“å®¶ã€‚\n",
    "è½®æ¬¡: ç¬¬{round_num}è½®\n",
    "ç”¨æˆ·éœ€æ±‚: {user_requirements}\n",
    "ä¸Šä¸€è½®è¯„åˆ†(è‹¥æœ‰): {last_scores}\n",
    "é•¿ç¨‹è®°å¿†æ£€ç´¢ç‰‡æ®µ:\n",
    "{memory_snippets}\n",
    "å·¥å…·è§‚å¯Ÿ:\n",
    "{tool_observations}\n",
    "å¾…ä¼˜åŒ–æ–‡æœ¬:\n",
    "{text_to_optimize}\n",
    "{previous_feedback}\n",
    "\n",
    "è¯·è¾“å‡ºï¼š\n",
    "**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**\n",
    "[ä¼˜åŒ–åçš„å®Œæ•´æ–‡æœ¬]\n",
    "\n",
    "**ä¿®æ”¹è¯´æ˜ï¼š**\n",
    "[è¯´æ˜æœ¬è½®ä¿®æ”¹è¦ç‚¹ï¼Œå°¤å…¶é’ˆå¯¹è¯„å®¡æå‡ºçš„é«˜ä¼˜å…ˆçº§é—®é¢˜]\n",
    "\"\"\"\n",
    "        )\n",
    "        self.agent_b_template = PromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "ä½ æ˜¯Agent B - å­¦æœ¯è¯„å®¡ä¸å¯¹æŠ—è´¨è¯¢ä¸“å®¶ã€‚\n",
    "è½®æ¬¡: ç¬¬{round_num}è½®\n",
    "ç”¨æˆ·éœ€æ±‚: {user_requirements}\n",
    "ä¼˜åŒ–æ–‡æœ¬:\n",
    "{optimized_text}\n",
    "\n",
    "è¯·è¯„å®¡å¹¶è¾“å‡º(ä¸¥æ ¼åŒ…å«ä»¥ä¸‹æ¿å—ä¸æ•°å€¼)ï¼š\n",
    "**æœ¬è½®æ”¹è¿›è¯„ä»·ï¼š**\n",
    "[æ€»ä½“è¯„ä»·]\n",
    "\n",
    "**è¯„åˆ†(è¯·ä½¿ç”¨JSONæ ¼å¼)**\n",
    "{\"quality\": <1-10>, \"rigor\": <1-10>, \"logic\": <1-10>, \"novelty\": <1-10>, \"priority_issues\": <æè¿°>}\n",
    "\n",
    "**å‰©ä½™ä¸»è¦é—®é¢˜ï¼š**\n",
    "[...]\n",
    "\n",
    "**ä¸‹è½®é‡ç‚¹å»ºè®®ï¼š**\n",
    "1. [...]\n",
    "2. [...]\n",
    "\n",
    "**æ”¹è¿›ä¼˜å…ˆçº§ï¼š**\n",
    "[é«˜/ä¸­/ä½ åˆ†å±‚åˆ—å‡º]\n",
    "\"\"\"\n",
    "        )\n",
    "        self.agent_a_chain = self.agent_a_template | self.agent_a_llm | StrOutputParser()\n",
    "        self.agent_b_chain = self.agent_b_template | self.agent_b_llm | StrOutputParser()\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_section(text: str, start_token: str, end_token: str) -> str:\n",
    "        lines = text.split('\\n')\n",
    "        collecting = False\n",
    "        buf = []\n",
    "        for l in lines:\n",
    "            if start_token in l:\n",
    "                collecting = True\n",
    "                continue\n",
    "            if collecting and end_token in l:\n",
    "                break\n",
    "            if collecting:\n",
    "                buf.append(l)\n",
    "        return '\\n'.join(buf).strip()\n",
    "\n",
    "    def _compute_diff(self, prev: str, current: str) -> str:\n",
    "        if prev is None:\n",
    "            return '(é¦–è½®æ— diff)'\n",
    "        import difflib as _df\n",
    "        diff_lines = _df.unified_diff(prev.splitlines(), current.splitlines(), lineterm='')\n",
    "        collected = []\n",
    "        for i, line in enumerate(diff_lines):\n",
    "            if i > 400:\n",
    "                collected.append('... <diff truncated>')\n",
    "                break\n",
    "            collected.append(line)\n",
    "        return '\\n'.join(collected) if collected else '(æ— å˜åŒ–)'\n",
    "\n",
    "    def _parse_scores(self, feedback: str) -> Dict[str, float]:\n",
    "        import json as _json, re as _re\n",
    "        m = _re.search(r'\\{\\s*\"quality\".*?}', feedback, flags=_re.S)\n",
    "        if not m:\n",
    "            return {}\n",
    "        try:\n",
    "            data = _json.loads(m.group(0))\n",
    "            for k in [\"quality\", \"rigor\", \"logic\", \"novelty\"]:\n",
    "                if k in data:\n",
    "                    data[k] = float(data[k])\n",
    "            return data\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    def _plan_and_act(self, text: str, requirements: List[str]) -> str:\n",
    "        if not self.tools_enabled:\n",
    "            return '(å·¥å…·å·²ç¦ç”¨)'\n",
    "        observations = []\n",
    "        joined_req = ' '.join(requirements).lower()\n",
    "        # æœç´¢è§¦å‘é€»è¾‘\n",
    "        if any(kw in joined_req for kw in [\"search\", \"æ£€ç´¢\", \"äº‹å®\", \"æœ€æ–°\", \"å¼•ç”¨\"]):\n",
    "            import re as _re2\n",
    "            m = _re2.findall(r'\"([^\\\"]+)\"', text)\n",
    "            query = m[-1] if m else text.split('ã€‚')[-1].strip() or text\n",
    "            try:\n",
    "                obs = self.tools[\"ç½‘ç»œæœç´¢\"].run(query)\n",
    "            except Exception as e:\n",
    "                obs = f\"[æœç´¢å¼‚å¸¸: {e}]\"\n",
    "            observations.append(f\"æœç´¢[{query}] -> {str(obs)[:300]}\")\n",
    "        # Python ä»£ç å—æ‰§è¡Œ (åªæ‰§è¡Œé¦–ä¸ªï¼Œé˜²æ­¢è¿‡åº¦è°ƒç”¨)\n",
    "        import re as _re3\n",
    "        code_blocks = _re3.findall(r'```python\\n([\\s\\S]*?)```', text, flags=_re3.IGNORECASE)\n",
    "        if code_blocks:\n",
    "            code = code_blocks[0]\n",
    "            try:\n",
    "                out = self.tools.get(\"Python REPL\").run(code)\n",
    "            except Exception as e:\n",
    "                out = f\"[ä»£ç æ‰§è¡Œå¼‚å¸¸: {e}]\"\n",
    "            observations.append(f\"æ‰§è¡ŒPython -> è¾“å‡º: {str(out)[:200]}\")\n",
    "        return '\\n'.join(observations) if observations else '(æ— )'\n",
    "\n",
    "    # ===== åŸºç¡€ä¸­æ–‡åˆ†è¯ & å¯è¯»æ€§/è¿è´¯æ€§ä»£ç†æŒ‡æ ‡ =====\n",
    "    @staticmethod\n",
    "    def _tokenize_zh(text: str) -> List[str]:\n",
    "        import re as _re4\n",
    "        return _re4.findall(r\"[\\u4e00-\\u9fff]|[A-Za-z0-9_]+\", text)\n",
    "\n",
    "    def _readability_proxy(self, text: str) -> float:\n",
    "        import re as _re5\n",
    "        sentences = [s for s in _re5.split(r'[ã€‚.!?]\\s*', text) if s.strip()]\n",
    "        if not sentences:\n",
    "            return 0.0\n",
    "        avg_len = sum(len(s) for s in sentences) / len(sentences)\n",
    "        return round(1 / (1 + avg_len / 50), 4)\n",
    "\n",
    "    def _coherence_proxy(self, text: str) -> float:\n",
    "        import re as _re6\n",
    "        sentences = [s for s in _re6.split(r'[ã€‚.!?]\\s*', text) if s.strip()]\n",
    "        if len(sentences) < 2:\n",
    "            return 0.0\n",
    "        def toks(s):\n",
    "            return set(self._tokenize_zh(s))\n",
    "        overlaps = []\n",
    "        for a, b in zip(sentences[:-1], sentences[1:]):\n",
    "            ta, tb = toks(a), toks(b)\n",
    "            if ta and tb:\n",
    "                overlaps.append(len(ta & tb) / len(ta | tb))\n",
    "        return round(sum(overlaps) / len(overlaps), 4) if overlaps else 0.0\n",
    "\n",
    "    # ===== ä¸»åä½œæµç¨‹ =====\n",
    "    def collaborate(self, user_text: str, user_requirements: List[str], language: str = \"ä¸­æ–‡\", rounds: int = 3) -> Tuple[str, List[Dict]]:\n",
    "        self.collaboration_log = [{\"round\": 0, \"user_input\": user_text, \"requirements\": user_requirements, \"timestamp\": datetime.now().isoformat()}]\n",
    "        current_text = user_text\n",
    "        previous_feedback = \"\"\n",
    "        last_scores: Dict[str, float] = {}\n",
    "        if self.memory_enabled:\n",
    "            self.memory.add_memory(user_text, {\"type\": \"user_input\"})\n",
    "        for r in range(1, rounds + 1):\n",
    "            mem_snippets = []\n",
    "            if self.memory_enabled:\n",
    "                mem_snippets = self.memory.recall(current_text, k=3)\n",
    "            tool_obs = self._plan_and_act(current_text, user_requirements)\n",
    "            a_input = {\n",
    "                \"round_num\": r,\n",
    "                \"text_to_optimize\": current_text,\n",
    "                \"user_requirements\": ', '.join(user_requirements),\n",
    "                \"previous_feedback\": previous_feedback,\n",
    "                \"memory_snippets\": '\\n'.join(mem_snippets) if mem_snippets else '(æ— )',\n",
    "                \"tool_observations\": tool_obs,\n",
    "                \"last_scores\": last_scores if last_scores else '(æ— )'\n",
    "            }\n",
    "            a_resp = self.agent_a_chain.invoke(a_input)\n",
    "            optimized_text = self._extract_section(a_resp, \"**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**\", \"**ä¿®æ”¹è¯´æ˜ï¼š**\") or current_text\n",
    "            b_input = {\n",
    "                \"round_num\": r,\n",
    "                \"optimized_text\": optimized_text,\n",
    "                \"user_requirements\": ', '.join(user_requirements)\n",
    "            }\n",
    "            b_resp = self.agent_b_chain.invoke(b_input)\n",
    "            last_scores = self._parse_scores(b_resp)\n",
    "            diff_str = self._compute_diff(current_text, optimized_text)\n",
    "            if self.memory_enabled:\n",
    "                self.memory.add_memory(optimized_text, {\"type\": \"optimized_text\", \"round\": r})\n",
    "                self.memory.add_memory(b_resp, {\"type\": \"feedback\", \"round\": r})\n",
    "            self.collaboration_log.append({\n",
    "                \"round\": r,\n",
    "                \"agent_a_response\": a_resp,\n",
    "                \"optimized_text\": optimized_text,\n",
    "                \"agent_b_feedback\": b_resp,\n",
    "                \"scores\": last_scores,\n",
    "                \"tool_observations\": tool_obs,\n",
    "                \"diff\": diff_str,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "            previous_feedback = b_resp\n",
    "            current_text = optimized_text\n",
    "            print(f\"âœ… Round {r} å®Œæˆ | è¯„åˆ†: {last_scores if last_scores else '{}'}\")\n",
    "            time.sleep(0.15)\n",
    "        return current_text, self.collaboration_log\n",
    "\n",
    "    # ===== æ•°æ®åˆæˆ (ç”¨äºè’¸é¦æ•™å¸ˆä¿¡å·) =====\n",
    "    def synthesize_dataset(self, seeds: List[str], requirements: List[str], rounds: int = 3,\n",
    "                           out_path: Optional[str] = None) -> str:\n",
    "        from pathlib import Path as _P\n",
    "        data_dir = _P(\"data\")\n",
    "        data_dir.mkdir(parents=True, exist_ok=True)\n",
    "        if out_path is None:\n",
    "            out_path = str(data_dir / f\"synth_academic_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl\")\n",
    "        count = 0\n",
    "        with open(out_path, 'w', encoding='utf-8') as f:\n",
    "            for i, seed in enumerate(seeds):\n",
    "                final_text, log = self.collaborate(seed, requirements, rounds=rounds)\n",
    "                record = {\n",
    "                    \"id\": f\"case_{i}\",\n",
    "                    \"input\": seed,\n",
    "                    \"requirements\": requirements,\n",
    "                    \"final\": final_text,\n",
    "                    \"log\": log,\n",
    "                    \"created_at\": datetime.now().isoformat(),\n",
    "                    \"teacher_signal\": log[-1].get(\"optimized_text\", final_text),\n",
    "                    \"scores\": log[-1].get(\"scores\", {})\n",
    "                }\n",
    "                import json as _json2\n",
    "                f.write(_json2.dumps(record, ensure_ascii=False) + '\\n')\n",
    "                count += 1\n",
    "        print(f\"ğŸ“¦ åˆæˆæ•°æ®å·²å†™å…¥: {out_path} (å…± {count} æ¡)\")\n",
    "        return out_path\n",
    "\n",
    "    # ===== è’¸é¦å¯¹å‡†å¤‡ =====\n",
    "    def prepare_distillation_pairs(self, jsonl_path: str, out_path: str) -> str:\n",
    "        import json as _json3\n",
    "        pairs = []\n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for ln in f:\n",
    "                if not ln.strip():\n",
    "                    continue\n",
    "                obj = _json3.loads(ln)\n",
    "                instr = f\"ä¼˜åŒ–ä»¥ä¸‹å­¦æœ¯æ®µè½, æ»¡è¶³éœ€æ±‚: {', '.join(obj.get('requirements', []))}\\nåŸæ–‡: {obj.get('input', '')}\"\n",
    "                target = obj.get('teacher_signal', obj.get('final', ''))\n",
    "                scores = obj.get('scores', {})\n",
    "                pairs.append({\"instruction\": instr, \"output\": target, \"scores\": scores})\n",
    "        with open(out_path, 'w', encoding='utf-8') as w:\n",
    "            for p in pairs:\n",
    "                w.write(_json3.dumps(p, ensure_ascii=False) + '\\n')\n",
    "        print(f\"ğŸ§ª è’¸é¦æ•°æ®å·²ç”Ÿæˆ: {out_path} å…± {len(pairs)} æ¡\")\n",
    "        return out_path\n",
    "\n",
    "    # ===== è¯„ä¼°é€»è¾‘ (å« AcademicMetrics é›†æˆ) =====\n",
    "    def evaluate(self, cases: List[Tuple[str, List[str]]], rounds: int = 2) -> Dict:\n",
    "        import json as _json4\n",
    "        from collections import Counter\n",
    "        results = []\n",
    "        for idx, (text, reqs) in enumerate(cases):\n",
    "            final_text, log = self.collaborate(text, reqs, rounds=rounds)\n",
    "            w0 = self._tokenize_zh(text)\n",
    "            w1 = self._tokenize_zh(final_text)\n",
    "            len_gain = (len(w1) - len(w0)) / max(1, len(w0))\n",
    "            ttr0 = len(set(w0)) / max(1, len(w0))\n",
    "            ttr1 = len(set(w1)) / max(1, len(w1))\n",
    "            c0 = Counter(w0)\n",
    "            c1 = Counter(w1)\n",
    "            rep0 = sum(x for _, x in c0.most_common(5)) / max(1, len(w0))\n",
    "            rep1 = sum(x for _, x in c1.most_common(5)) / max(1, len(w1))\n",
    "            readability_gain = self._readability_proxy(final_text) - self._readability_proxy(text)\n",
    "            coherence_gain = self._coherence_proxy(final_text) - self._coherence_proxy(text)\n",
    "            import statistics as _stats\n",
    "            def _sent_lens(t: str):\n",
    "                import re as _re7\n",
    "                sents = [s for s in _re7.split(r'[ã€‚.!?]\\s*', t) if s.strip()]\n",
    "                return [len(s) for s in sents] if sents else []\n",
    "            var0 = _stats.pvariance(_sent_lens(text)) if _sent_lens(text) else 0.0\n",
    "            var1 = _stats.pvariance(_sent_lens(final_text)) if _sent_lens(final_text) else 0.0\n",
    "            var_delta = round(var0 - var1, 3)\n",
    "            def _bigram_rep(toks: List[str]):\n",
    "                bigrams = [tuple(toks[i:i+2]) for i in range(len(toks)-1)]\n",
    "                bc = Counter(bigrams)\n",
    "                total = len(bigrams) or 1\n",
    "                top = sum(v for _, v in bc.most_common(5))\n",
    "                return top / total\n",
    "            bigram_delta = round(_bigram_rep(w0) - _bigram_rep(w1), 3)\n",
    "            last_scores = log[-1].get(\"scores\", {}) if log else {}\n",
    "            advanced_metrics = {}\n",
    "            if AcademicMetrics:\n",
    "                try:\n",
    "                    original_eval = AcademicMetrics.overall_quality_score(text)\n",
    "                    optimized_eval = AcademicMetrics.overall_quality_score(final_text)\n",
    "                    orig_scores = (original_eval or {}).get('scores', {})\n",
    "                    opt_scores = (optimized_eval or {}).get('scores', {})\n",
    "                    advanced_metrics = {\n",
    "                        'original_overall_score': float((original_eval or {}).get('overall_score', 0.0)),\n",
    "                        'optimized_overall_score': float((optimized_eval or {}).get('overall_score', 0.0)),\n",
    "                        'academic_formality_improvement': round(float(opt_scores.get('academic_formality', 0.0)) - float(orig_scores.get('academic_formality', 0.0)), 4),\n",
    "                        'citation_completeness_improvement': round(float(opt_scores.get('citation_completeness', 0.0)) - float(orig_scores.get('citation_completeness', 0.0)), 4),\n",
    "                        'novelty_improvement': round(float(opt_scores.get('novelty', 0.0)) - float(orig_scores.get('novelty', 0.0)), 4),\n",
    "                        'language_fluency_improvement': round(float(opt_scores.get('language_fluency', 0.0)) - float(orig_scores.get('language_fluency', 0.0)), 4),\n",
    "                        'sentence_balance_improvement': round(float(opt_scores.get('sentence_balance', 0.0)) - float(orig_scores.get('sentence_balance', 0.0)), 4),\n",
    "                        'argumentation_improvement': round(float(opt_scores.get('argumentation', 0.0)) - float(orig_scores.get('argumentation', 0.0)), 4),\n",
    "                        'expression_diversity_improvement': round(float(opt_scores.get('expression_diversity', 0.0)) - float(orig_scores.get('expression_diversity', 0.0)), 4),\n",
    "                        'structure_completeness_improvement': round(float(opt_scores.get('structure_completeness', 0.0)) - float(orig_scores.get('structure_completeness', 0.0)), 4),\n",
    "                        'tense_consistency_improvement': round(float(opt_scores.get('tense_consistency', 0.0)) - float(orig_scores.get('tense_consistency', 0.0)), 4),\n",
    "                    }\n",
    "                except Exception as e:\n",
    "                    print(f\"âš ï¸ è®¡ç®—é«˜çº§æŒ‡æ ‡å¤±è´¥: {e}\")\n",
    "            results.append({\n",
    "                \"id\": idx,\n",
    "                \"len_gain\": round(len_gain, 3),\n",
    "                \"ttr_gain\": round(ttr1 - ttr0, 3),\n",
    "                \"repetition_delta\": round(rep0 - rep1, 3),\n",
    "                \"readability_gain\": round(readability_gain, 3),\n",
    "                \"coherence_gain\": round(coherence_gain, 3),\n",
    "                \"sent_var_delta\": var_delta,\n",
    "                \"bigram_rep_delta\": round(bigram_delta, 3),\n",
    "                \"orig_len\": len(w0),\n",
    "                \"final_len\": len(w1),\n",
    "                \"scores\": last_scores,\n",
    "                \"advanced_metrics\": advanced_metrics\n",
    "            })\n",
    "        if results:\n",
    "            avg = {\n",
    "                \"len_gain_avg\": round(sum(r[\"len_gain\"] for r in results) / len(results), 3),\n",
    "                \"ttr_gain_avg\": round(sum(r[\"ttr_gain\"] for r in results) / len(results), 3),\n",
    "                \"repetition_delta_avg\": round(sum(r[\"repetition_delta\"] for r in results) / len(results), 3),\n",
    "                \"readability_gain_avg\": round(sum(r[\"readability_gain\"] for r in results) / len(results), 3),\n",
    "                \"coherence_gain_avg\": round(sum(r[\"coherence_gain\"] for r in results) / len(results), 3),\n",
    "                \"sent_var_delta_avg\": round(sum(r[\"sent_var_delta\"] for r in results) / len(results), 3),\n",
    "                \"bigram_rep_delta_avg\": round(sum(r[\"bigram_rep_delta\"] for r in results) / len(results), 3),\n",
    "                \"quality_avg\": round(sum(r.get(\"scores\", {}).get(\"quality\", 0) for r in results) / len(results), 3),\n",
    "                \"rigor_avg\": round(sum(r.get(\"scores\", {}).get(\"rigor\", 0) for r in results) / len(results), 3),\n",
    "                \"logic_avg\": round(sum(r.get(\"scores\", {}).get(\"logic\", 0) for r in results) / len(results), 3),\n",
    "                \"novelty_avg\": round(sum(r.get(\"scores\", {}).get(\"novelty\", 0) for r in results) / len(results), 3),\n",
    "                \"n\": len(results)\n",
    "            }\n",
    "            if results[0].get(\"advanced_metrics\"):\n",
    "                adv_avg = {}\n",
    "                for metric in results[0][\"advanced_metrics\"].keys():\n",
    "                    adv_avg[f\"{metric}_avg\"] = round(sum(r.get(\"advanced_metrics\", {}).get(metric, 0) for r in results) / len(results), 4)\n",
    "                avg.update(adv_avg)\n",
    "        else:\n",
    "            avg = {\"len_gain_avg\": 0, \"ttr_gain_avg\": 0, \"repetition_delta_avg\": 0, \"readability_gain_avg\": 0, \"coherence_gain_avg\": 0, \"sent_var_delta_avg\": 0, \"bigram_rep_delta_avg\": 0, \"quality_avg\": 0, \"rigor_avg\": 0, \"logic_avg\": 0, \"novelty_avg\": 0, \"n\": 0}\n",
    "        report = {\"summary\": avg, \"cases\": results}\n",
    "        print(\"ğŸ“Š è¯„ä¼°æ±‡æ€»:\", json.dumps(report[\"summary\"], ensure_ascii=False))\n",
    "        return report\n",
    "\n",
    "    # ===== é•¿æ–‡æœ¬åˆ†æ®µå¤„ç† =====\n",
    "    def _split_long_text(self, text: str, chunk_size: int, overlap: int) -> List[str]:\n",
    "        text = text.strip()\n",
    "        if chunk_size <= 0:\n",
    "            return [text]\n",
    "        import re as _re8\n",
    "        sentences = _re8.split(r'([ã€‚.!?]\\s*)', text)\n",
    "        buf = ''\n",
    "        chunks = []\n",
    "        for i in range(0, len(sentences), 2):\n",
    "            seg = sentences[i]\n",
    "            delim = sentences[i + 1] if i + 1 < len(sentences) else ''\n",
    "            piece = seg + delim\n",
    "            if len(buf) + len(piece) <= chunk_size:\n",
    "                buf += piece\n",
    "            else:\n",
    "                if buf:\n",
    "                    chunks.append(buf)\n",
    "                buf = piece\n",
    "        if buf:\n",
    "            chunks.append(buf)\n",
    "        if overlap > 0 and len(chunks) > 1:\n",
    "            with_overlap = []\n",
    "            prev_tail = ''\n",
    "            for idx, c in enumerate(chunks):\n",
    "                if idx == 0:\n",
    "                    with_overlap.append(c)\n",
    "                else:\n",
    "                    tail = prev_tail[-overlap:] if overlap < len(prev_tail) else prev_tail\n",
    "                    with_overlap.append((tail + c).strip())\n",
    "                prev_tail = c\n",
    "            chunks = with_overlap\n",
    "        return chunks\n",
    "\n",
    "    def optimize_text_file(self, file_path: str, requirements: List[str], rounds: int,\n",
    "                           chunk_size: int, overlap: int, max_chunks: int = 0) -> Tuple[str, Dict]:\n",
    "        from pathlib import Path as _P2\n",
    "        p = _P2(file_path)\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f'æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')\n",
    "        raw = p.read_text(encoding='utf-8')\n",
    "        chunks = self._split_long_text(raw, chunk_size, overlap)\n",
    "        if max_chunks > 0:\n",
    "            chunks = chunks[:max_chunks]\n",
    "        optimized_segments = []\n",
    "        segment_logs = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            print(f'ğŸ§© å¤„ç†åˆ†æ®µ {idx+1}/{len(chunks)} (é•¿åº¦={len(chunk)})')\n",
    "            final_seg, log = self.collaborate(chunk, requirements, rounds=rounds)\n",
    "            optimized_segments.append(final_seg)\n",
    "            segment_logs.append({\n",
    "                'segment_index': idx,\n",
    "                'original_length': len(chunk),\n",
    "                'optimized_length': len(final_seg),\n",
    "                'final_segment_text': final_seg,\n",
    "                'round_logs': log\n",
    "            })\n",
    "        combined_final = '\\n\\n'.join(optimized_segments)\n",
    "        aggregated = {\n",
    "            'file': file_path,\n",
    "            'chunks': len(chunks),\n",
    "            'chunk_size': chunk_size,\n",
    "            'overlap': overlap,\n",
    "            'requirements': requirements,\n",
    "            'final_text': combined_final,\n",
    "            'segments': segment_logs,\n",
    "        }\n",
    "        return combined_final, aggregated\n",
    "\n",
    "    # ===== HTML æŠ¥å‘Šç”Ÿæˆ =====\n",
    "    def generate_html_report(self, title: str, final_text: str, log: List[Dict], summary: Optional[Dict] = None) -> str:\n",
    "        def style_block():\n",
    "            return '''<style>body{font-family:Segoe UI,Arial,sans-serif;max-width:1080px;margin:32px auto;line-height:1.5}pre{background:#fafafa;border:1px solid #eee;padding:8px;white-space:pre-wrap}table{border-collapse:collapse;width:100%;margin:10px 0}td,th{border:1px solid #ccc;padding:6px 10px;font-size:13px}th{background:#f0f0f0} .score-badge{display:inline-block;padding:3px 8px;border-radius:4px;background:#004d7a;color:#fff;font-size:12px;margin:0 4px 4px 0} .diff-add{background:#e6ffe6} .diff-del{background:#ffecec;color:#900} .round-box{border:1px solid #ddd;padding:12px;margin:14px 0;border-radius:6px;background:#fafafa} .metric-value{font-weight:bold;color:#2c3e50}</style>'''\n",
    "        def render_scores(scores: Dict[str, float]) -> str:\n",
    "            if not scores:\n",
    "                return '<span style=\"color:#777\">æ— è¯„åˆ†</span>'\n",
    "            return ' '.join(f'<span class=\"score-badge\">{k}:{v:.1f}</span>' for k,v in scores.items() if isinstance(v,(int,float)))\n",
    "        def color_diff(diff_text: str) -> str:\n",
    "            lines = []\n",
    "            for ln in diff_text.splitlines():\n",
    "                if ln.startswith('+') and not ln.startswith('+++'):\n",
    "                    lines.append(f'<div class=\"diff-add\">{_html_escape(ln)}</div>')\n",
    "                elif ln.startswith('-') and not ln.startswith('---'):\n",
    "                    lines.append(f'<div class=\"diff-del\">{_html_escape(ln)}</div>')\n",
    "                else:\n",
    "                    lines.append(f'<div>{_html_escape(ln)}</div>')\n",
    "            return '\\n'.join(lines)\n",
    "        parts = [f'<html><head><meta charset=\"utf-8\"><title>{_html_escape(title)}</title>{style_block()}</head><body>']\n",
    "        parts.append(f'<h1>ğŸ“ˆ {_html_escape(title)}</h1>')\n",
    "        parts.append(f'<div style=\"color:#555;font-size:12px\">ç”Ÿæˆæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}</div>')\n",
    "        if summary:\n",
    "            parts.append('<h2>ğŸ“Š æŒ‡æ ‡æ±‡æ€»</h2><table><tr><th>é•¿åº¦å˜åŒ–</th><th>è¯æ±‡å¤šæ ·æ€§æå‡</th><th>é‡å¤åº¦ä¸‹é™</th><th>å¯è¯»æ€§æå‡</th><th>è¿è´¯æ€§æå‡</th></tr>')\n",
    "            parts.append('<tr>' + ''.join(f'<td>{summary.get(k,0):.4f}</td>' for k in [\"len_gain_avg\",\"ttr_gain_avg\",\"repetition_delta_avg\",\"readability_gain_avg\",\"coherence_gain_avg\"]) + '</tr></table>')\n",
    "        parts.append('<h2>ğŸ“ æœ€ç»ˆä¼˜åŒ–æ–‡æœ¬</h2><pre>' + _html_escape(final_text) + '</pre>')\n",
    "        parts.append('<h2>ğŸ“‚ è½®æ¬¡è¯¦ç»†æ—¥å¿—</h2>')\n",
    "        for entry in log[1:]:\n",
    "            parts.append('<div class=\"round-box\">')\n",
    "            parts.append(f'<h3>Round {entry.get(\"round\")}</h3>')\n",
    "            parts.append(f'<div style=\"color:#666;font-size:12px\">æ—¶é—´: {entry.get(\"timestamp\")}</div>')\n",
    "            parts.append('<h4>âœï¸ ä¼˜åŒ–æ–‡æœ¬</h4><pre>' + _html_escape(entry.get('optimized_text','')) + '</pre>')\n",
    "            parts.append('<h4>ğŸ—’ Agent B åé¦ˆ</h4><pre>' + _html_escape(entry.get('agent_b_feedback','')) + '</pre>')\n",
    "            parts.append('<h4>â­ è¯„åˆ†</h4>' + render_scores(entry.get('scores',{})))\n",
    "            parts.append('<details><summary>ğŸ” æ–‡æœ¬å·®å¼‚ (Diff)</summary>' + color_diff(entry.get('diff','')) + '</details>')\n",
    "            if entry.get('tool_observations') and entry.get('tool_observations') not in ('(æ— )','(å·¥å…·å·²ç¦ç”¨)'):\n",
    "                parts.append('<details><summary>ğŸ›  å·¥å…·è°ƒç”¨è§‚å¯Ÿ</summary><pre>' + _html_escape(entry.get('tool_observations','')) + '</pre></details>')\n",
    "            parts.append('</div>')\n",
    "        parts.append('</body></html>')\n",
    "        return '\\n'.join(parts)\n",
    "\n",
    "# æ„å»ºå¯é€‰æ··åˆç³»ç»Ÿï¼ˆå­¦ç”Ÿæ¨¡å‹ + æ•™å¸ˆæ¨¡å‹ï¼‰\n",
    "try:\n",
    "    from hf_student_llm import HFChatLLM as _HFStudentLLM\n",
    "except Exception:\n",
    "    _HFStudentLLM = None\n",
    "\n",
    "def build_hybrid_dual_agent_system(base_llm) -> DualAgentAcademicSystem:\n",
    "    if _HFStudentLLM is None:\n",
    "        print(\"âš ï¸ HF å­¦ç”Ÿæ¨¡å‹ä¸å¯ç”¨ï¼ˆç¼ºå°‘ transformers/peft æˆ–å¯¼å…¥å¤±è´¥ï¼‰ï¼Œä½¿ç”¨å•ä¸€ LLM æ¨¡å¼ã€‚\")\n",
    "        return DualAgentAcademicSystem(base_llm, TOOLS, vectorstore)\n",
    "    import os as _os\n",
    "    from pathlib import Path as _P3\n",
    "    base_model = _os.getenv(\"STUDENT_BASE_MODEL\", \"Qwen/Qwen1.5-1.8B-Chat\")\n",
    "    lora_dir = _os.getenv(\"STUDENT_LORA_DIR\")\n",
    "    max_new = int(_os.getenv(\"STUDENT_MAX_NEW_TOKENS\", \"512\"))\n",
    "    if not lora_dir or not _P3(lora_dir).exists():\n",
    "        print(f\"âš ï¸ STUDENT_LORA_DIR æœªè®¾ç½®æˆ–è·¯å¾„ä¸å­˜åœ¨ ({lora_dir})ï¼Œä»…åŠ è½½åŸºç¡€æ¨¡å‹ã€‚\")\n",
    "        lora_dir = \"\"\n",
    "    try:\n",
    "        if lora_dir:\n",
    "            student_llm = _HFStudentLLM(base_model=base_model, lora_dir=lora_dir, max_new_tokens=max_new)\n",
    "        else:\n",
    "            student_llm = _HFStudentLLM(base_model=base_model, lora_dir=base_model, max_new_tokens=max_new)\n",
    "        print(f\"ğŸ¤– Agent A ä½¿ç”¨æœ¬åœ°å­¦ç”Ÿæ¨¡å‹: {base_model} (LoRA: {bool(lora_dir)}) | Agent B ä½¿ç”¨è¿œç¨‹æ•™å¸ˆæ¨¡å‹\")\n",
    "        return DualAgentAcademicSystem(base_llm, TOOLS, vectorstore, agent_a_llm=student_llm, agent_b_llm=base_llm)\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ åˆå§‹åŒ–å­¦ç”Ÿæ¨¡å‹å¤±è´¥: {e}ï¼Œé€€å›å•ä¸€ LLM æ¨¡å¼ã€‚\")\n",
    "        return DualAgentAcademicSystem(base_llm, TOOLS, vectorstore)\n",
    "\n",
    "# é‡æ–°åˆå§‹åŒ–ç³»ç»Ÿä¸ºæ··åˆï¼ˆè‹¥å­¦ç”Ÿæ¨¡å‹å¯ç”¨ï¼‰\n",
    "dual_agent_system = build_hybrid_dual_agent_system(llm)\n",
    "print(\"ğŸ¤– åŒAgentç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼ˆæ”¯æŒ Hybrid å­¦ç”Ÿ/æ•™å¸ˆ æ¨¡å¼ï¼›è‹¥ä¸å¯ç”¨åˆ™å•æ¨¡å‹ï¼‰\")\n"
   ],
   "id": "2f576f7d25bb1062"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 6. Notebook ä¸­çš„ä½¿ç”¨ç¤ºä¾‹ï¼ˆåŸºç¡€åä½œæ¼”ç¤ºï¼‰\n",
    "#\n",
    "# å±•ç¤ºåŒ Agent åœ¨ 2 è½®å†…å¯¹åˆç¨¿è¿›è¡Œå­¦æœ¯ä¼˜åŒ–çš„åŸºæœ¬ç”¨æ³•ã€‚\n",
    "# å¯ä¿®æ”¹ `sample_text`ã€`requirements` æˆ– `rounds` è§‚å¯Ÿè¡Œä¸ºå˜åŒ–ã€‚\n"
   ],
   "id": "dcc837679bea301"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from pprint import pprint as _pprint_demo\n",
    "sample_text = \"è¿™æ˜¯ä¸€æ®µå…³äºå¤šæ™ºèƒ½ä½“åä½œè¿›è¡Œå­¦æœ¯å†™ä½œä¼˜åŒ–çš„åˆç¨¿ï¼Œè¡¨è¿°ç•¥æ˜¾å£è¯­åŒ–ï¼Œç»“æ„ä¹Ÿä¸å¤Ÿæ¸…æ™°ã€‚\"\n",
    "requirements = [\"å­¦æœ¯è¡¨è¾¾æå‡\", \"é€»è¾‘ç»“æ„ä¼˜åŒ–\"]\n",
    "final_text_demo, log_demo = dual_agent_system.collaborate(sample_text, requirements, rounds=2)\n",
    "print(\"\\nğŸ“Œ æœ€ç»ˆä¼˜åŒ–æ–‡æœ¬ (ç¤ºä¾‹):\\n\")\n",
    "print(final_text_demo)\n",
    "print(\"\\nğŸ“œ åä½œæ—¥å¿—æ‘˜è¦ï¼ˆæœ€åä¸€è½®ï¼‰:\\n\")\n",
    "_pprint_demo(log_demo[-1])\n"
   ],
   "id": "dda2ce0bfedfa7b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 7. é«˜çº§åŠŸèƒ½ï¼šæ•°æ®åˆæˆ / è¯„ä¼° / è’¸é¦ / é•¿æ–‡æœ¬ä¼˜åŒ– / HTML æŠ¥å‘Š\n",
    "#\n",
    "# æœ¬èŠ‚æä¾›ä¸è„šæœ¬ç‰ˆæœ¬åŒæ­¥çš„é«˜çº§æ¥å£ï¼š\n",
    "# - dual_agent_system.synthesize_dataset(seeds, requirements, rounds)\n",
    "# - dual_agent_system.evaluate([(text, reqs), ...], rounds)\n",
    "# - dual_agent_system.prepare_distillation_pairs(jsonl_in, jsonl_out)\n",
    "# - dual_agent_system.optimize_text_file(file_path, requirements, rounds, chunk_size, overlap)\n",
    "# - dual_agent_system.generate_html_report(title, final_text, log, summary)\n",
    "#\n",
    "# ä½ å¯ä»¥æ ¹æ®éœ€è¦åœ¨ä¸‹æ–¹æ·»åŠ ç¤ºä¾‹å•å…ƒæ ¼è¿›è¡Œè°ƒç”¨ã€‚\n"
   ],
   "id": "cca0d4fa4edf8d82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 8. ç¤ºä¾‹ï¼šè¯„ä¼°ä¸ HTML æŠ¥å‘Šç”Ÿæˆï¼ˆå¯è‡ªè¡Œå–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰\n",
    "# ```python\n",
    "# samples = [\n",
    "#     (\"è¿™æ˜¯ä¸€ä¸ªå…³äºå¤šæ™ºèƒ½ä½“åä½œçš„åˆç¨¿ã€‚\", [\"å­¦æœ¯è¡¨è¾¾æå‡\", \"é€»è¾‘ç»“æ„ä¼˜åŒ–\"]),\n",
    "#     (\"è¯¥æ–¹æ³•åœ¨å¤æ‚åœºæ™¯ä¸‹ä»å­˜åœ¨ç¨³å®šæ€§é—®é¢˜ã€‚\", [\"å­¦æœ¯è¡¨è¾¾æå‡\", \"ä¸¥è°¨æ€§å¢å¼º\"])\n",
    "# ]\n",
    "# report = dual_agent_system.evaluate(samples, rounds=2)\n",
    "# final_text, log = dual_agent_system.collaborate(\"å¤šæ™ºèƒ½ä½“ä¼˜åŒ–ç³»ç»Ÿå¯ä»¥æå‡å†™ä½œè´¨é‡ã€‚\", [\"å­¦æœ¯è¡¨è¾¾æå‡\"], rounds=2)\n",
    "# html = dual_agent_system.generate_html_report(\"ä¼˜åŒ–æŠ¥å‘Šç¤ºä¾‹\", final_text, log, report.get(\"summary\"))\n",
    "# open(\"report_example.html\", \"w\", encoding=\"utf-8\").write(html)\n",
    "# html[:500]\n",
    "# ```\n"
   ],
   "id": "7890d82b87aaee38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 9. ç¤ºä¾‹ï¼šé•¿æ–‡æœ¬åˆ†æ®µä¼˜åŒ–ï¼ˆå¯è‡ªè¡Œå–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰\n",
    "# ```python\n",
    "# long_text = \"\"\"ä½ çš„å¾ˆé•¿çš„å­¦æœ¯åˆç¨¿...\"\"\"\n",
    "# final_long, agg = dual_agent_system.optimize_text_file(\"path/to/long.txt\", [\"å­¦æœ¯è¡¨è¾¾æå‡\"], rounds=2, chunk_size=4000, overlap=200)\n",
    "# len(final_long), agg['chunks']\n",
    "# ```\n"
   ],
   "id": "6283fbf022ce0228"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 10. ç¤ºä¾‹ï¼šæ•°æ®åˆæˆä¸è’¸é¦å¯¹å‡†å¤‡ï¼ˆå¯è‡ªè¡Œå–æ¶ˆæ³¨é‡Šè¿è¡Œï¼‰\n",
    "# ```python\n",
    "# seeds = [\"åˆç¨¿æ®µè½ A\", \"åˆç¨¿æ®µè½ B\"]\n",
    "# ds_path = dual_agent_system.synthesize_dataset(seeds, [\"å­¦æœ¯è¡¨è¾¾æå‡\"], rounds=2)\n",
    "# dual_agent_system.prepare_distillation_pairs(ds_path, \"distill_pairs.jsonl\")\n",
    "# ```\n"
   ],
   "id": "8543d72ecffcaf02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
