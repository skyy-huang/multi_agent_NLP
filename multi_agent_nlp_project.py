import os
import time
from datetime import datetime
from typing import Dict, List, Tuple, Optional
from dotenv import load_dotenv
import json
import argparse
import re
import requests
import sys
from pathlib import Path  # é‡æ–°æ·»åŠ  Path å¯¼å…¥ä»¥ä¿®å¤ç±»å‹å¼•ç”¨

# åœ¨ä½ä¾èµ–ç¯å¢ƒä¸‹å›é€€ PromptTemplate ä¸ StrOutputParser
try:
    from langchain_core.prompts import PromptTemplate
    from langchain_core.output_parsers import StrOutputParser

    _PROMPT_AVAILABLE = True
except Exception as _e:
    print(f"âš ï¸ langchain_core prompts/output_parsers å¯¼å…¥å¤±è´¥({_e}); ä½¿ç”¨è½»é‡ stub")
    _PROMPT_AVAILABLE = False


    class PromptTemplate:
        def __init__(self, template: str):
            self.template = template

        @classmethod
        def from_template(cls, t: str):
            return cls(t)

        def format(self, **kwargs):
            return self.template.format(**kwargs)

        def __or__(self, other):  # é“¾æ¥æ“ä½œå…¼å®¹
            return other


    class StrOutputParser:
        def __or__(self, other):
            return other

        def invoke(self, x):  # ä¿ç•™æ¥å£
            return x

# ç§»é™¤é‡å¤çš„ Path å¯¼å…¥ï¼Œå¿…è¦æ—¶å†å±€éƒ¨å¯¼å…¥

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„ä»¥ä¾¿å¯¼å…¥ metrics æ¨¡å—
sys.path.insert(0, os.path.dirname(__file__))

try:
    from metrics import AcademicMetrics
except ImportError:
    print("âš ï¸ metrics æ¨¡å—å¯¼å…¥å¤±è´¥ï¼Œå°†ä½¿ç”¨åŸºç¡€æŒ‡æ ‡è®¡ç®—å ä½é€»è¾‘")
    AcademicMetrics = None

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
SERPAPI_API_KEY = os.getenv("SERPAPI_API_KEY")
OPENAI_BASE_URL = os.getenv("OPENAI_BASE_URL", "https://api.chatanywhere.tech/v1")
LLM_MODEL = os.getenv("LLM_MODEL", "gpt-4o-mini")
IS_DEEPSEEK = "deepseek.com" in (OPENAI_BASE_URL or "").lower()
# Normalize DeepSeek model naming per official docs
if IS_DEEPSEEK:
    lm_lower = (LLM_MODEL or "").lower()
    if lm_lower not in ("deepseek-chat", "deepseek-reasoner"):
        new_model = "deepseek-reasoner" if ("reason" in lm_lower or "think" in lm_lower) else "deepseek-chat"
        print(f"â„¹ï¸ æ£€æµ‹åˆ° DeepSeek ä¸”æ¨¡å‹å '{LLM_MODEL}' éå®˜æ–¹æ¨èï¼Œå·²è‡ªåŠ¨è§„èŒƒä¸º '{new_model}'ã€‚")
        LLM_MODEL = new_model


class DummyLLM:
    """Fallback LLM used when API keys are missing; mimics .invoke interface and is Runnable-compatible."""

    def __init__(self):
        self.model_name = "dummy-llm"

    def invoke(self, prompt: Dict | str):
        if isinstance(prompt, dict):
            return f"[DummyLLM response for keys: {list(prompt.keys())}]"
        return "[DummyLLM generic response]"

    def __call__(self, prompt: Dict | str):  # ä½¿å…¶å¯è°ƒç”¨, ä¾¿äº LangChain å°†å…¶è§†ä¸º Runnable
        return self.invoke(prompt)

    def __or__(self, other):  # åœ¨é“¾å¼æ“ä½œä¸­è¿”å›è‡ªèº«, è®© LangChain å¯¹ DummyLLM æ‰§è¡Œè°ƒç”¨
        return self


class HTTPFallbackChat:
    """
ç›´æ¥ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£çš„ç®€å•å›é€€å®¢æˆ·ç«¯ã€‚æ»¡è¶³ .invoke(dict|str) æ¥å£ã€‚
    """

    def __init__(self, base_url: str, api_key: str, model: str, timeout: float = 30.0):
        self.base_url = base_url.rstrip('/')
        self.api_key = api_key
        self.model = model
        self.timeout = timeout
        # å…¼å®¹æœ‰/æ—  /v1 å‰ç¼€
        if self.base_url.endswith('/v1'):
            self.endpoint = f"{self.base_url}/chat/completions"
        else:
            self.endpoint = f"{self.base_url}/v1/chat/completions"

    def invoke(self, prompt: Dict | str):
        if isinstance(prompt, dict):
            # å°†å­—å…¸å†…å®¹æ‹¼åˆä¸º user æ¶ˆ Messages
            user_content = '\n'.join(f"{k}: {v}" for k, v in prompt.items())
        else:
            user_content = str(prompt)
        payload = {
            "model": self.model,
            "messages": [
                {"role": "system", "content": "You are an academic writing optimization assistant."},
                {"role": "user", "content": user_content}
            ],
            "temperature": 0
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        try:
            resp = requests.post(self.endpoint, headers=headers, json=payload, timeout=self.timeout)
            if resp.status_code != 200:
                return f"[HTTPFallbackChat Error {resp.status_code}: {resp.text[:200]}]"
            data = resp.json()
            # OpenAI å…¼å®¹æ ¼å¼ï¼šchoices[0].message.content
            return data.get("choices", [{}])[0].get("message", {}).get("content", "[No content]")
        except Exception as e:
            return f"[HTTPFallbackChat Exception: {e}]"

    def __call__(self, prompt: Dict | str):  # å¢åŠ ç›´æ¥è°ƒç”¨æ”¯æŒ
        return self.invoke(prompt)

    def __or__(self, other):
        return other


def init_llm():
    if not OPENAI_API_KEY:
        print("âš ï¸ OPENAI_API_KEY missing; using DummyLLM.")
        return DummyLLM()
    # ä¼˜å…ˆå°è¯• langchain ChatOpenAI
    try:
        from langchain_openai import ChatOpenAI  # lazy import
        llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=0,
            api_key=(lambda: OPENAI_API_KEY),
            base_url=OPENAI_BASE_URL,
        )
        print(f"âœ… Primary LLM ({LLM_MODEL}) initialized via {OPENAI_BASE_URL}.")
        return llm
    except Exception as e:
        print(f"âŒ Failed to init ChatOpenAI: {e}; attempting HTTP fallback.")
        try:
            fallback_llm = HTTPFallbackChat(OPENAI_BASE_URL, OPENAI_API_KEY, LLM_MODEL)
            # ç®€å•æ¢é’ˆè°ƒç”¨ç¡®è®¤å¯ç”¨
            probe = fallback_llm.invoke("probe")
            if probe.startswith("[HTTPFallbackChat Error") or probe.startswith("[HTTPFallbackChat Exception"):
                print("âš ï¸ HTTP fallback probe failed; using DummyLLM.")
                return DummyLLM()
            print(f"âœ… HTTP Fallback LLM ({LLM_MODEL}) ready via {OPENAI_BASE_URL}.")
            return fallback_llm
        except Exception as e2:
            print(f"âŒ Fallback initialization failed: {e2}; using DummyLLM.")
            return DummyLLM()


llm = init_llm()

# LangChain / å·¥å…·å±‚è½¯å›é€€ï¼šè‹¥ä¾èµ–ç¼ºå¤±åˆ™ä½¿ç”¨è½»é‡å ä½å®ç°ä¿æŒæ¥å£å…¼å®¹
try:
    from langchain_core.tools import Tool  # type: ignore
    from langchain_experimental.utilities import PythonREPL  # type: ignore
    from langchain_community.utilities import SerpAPIWrapper  # type: ignore

    _LC_AVAILABLE = True
except Exception as e:
    print(f"âš ï¸ LangChain ç›¸å…³ä¾èµ–ç¼ºå¤±æˆ–å¯¼å…¥å¤±è´¥({e}); ä½¿ç”¨è½»é‡ stub æ›¿ä»£ï¼ŒåŠŸèƒ½å—é™ä½†æµç¨‹å¯ç»§ç»­ã€‚")
    _LC_AVAILABLE = False


    class Tool:  # minimal stub
        def __init__(self, name: str, func, description: str = ""):
            self.name = name
            self.func = func
            self.description = description

        def run(self, *args, **kwargs):
            return self.func(*args, **kwargs)


    class PythonREPL:  # stub REPL
        def run(self, code: str) -> str:
            try:
                local_env = {}
                exec(code, {}, local_env)
                return str(local_env)[:500]
            except Exception as e2:
                return f"[PythonREPL stub å¼‚å¸¸: {e2}]"


    class SerpAPIWrapper:  # stub search
        def __init__(self, *_, **__):
            pass

        def run(self, query: str) -> str:
            return f"[SerpAPI stub æœªé…ç½®ï¼Œæ— æ³•çœŸå®æ£€ç´¢: {query}]"

TOOLS: List[Tool] = []  # add explicit type for clarity
if SERPAPI_API_KEY:
    search_wrapper = SerpAPIWrapper(search_engine="google", serpapi_api_key=SERPAPI_API_KEY)
    search_tool = Tool(
        name="ç½‘ç»œæœç´¢",
        func=search_wrapper.run,
        description="å®æ—¶ä¿¡æ¯æŸ¥è¯¢ï¼šè¾“å…¥æœç´¢å…³é”®è¯"
    )
    TOOLS.append(search_tool)
else:
    def _search_stub(q: str) -> str:
        return f"[SerpAPI æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œæœç´¢: {q}]"


    search_tool = Tool(name="ç½‘ç»œæœç´¢", func=_search_stub, description="SerpAPI æœªé…ç½®å ä½å·¥å…·")
    TOOLS.append(search_tool)

python_repl = PythonREPL()
python_repl_tool = Tool(
    name="Python REPL",
    func=python_repl.run,
    description="æ‰§è¡Œæ ¼å¼æ­£ç¡®çš„ Python ä»£ç "
)
TOOLS.extend([
    python_repl_tool,
    # ç®€åŒ–æ–‡ä»¶è¯»å†™å·¥å…·ï¼Œé¿å…ä¾èµ–æœªå®šä¹‰çš„ read_file/write_file
    Tool(
        name="è¯»å–æ–‡ä»¶",
        func=lambda fn: open(fn, "r", encoding="utf-8").read(),
        description="è¯»å–æŒ‡å®šæ–‡æœ¬æ–‡ä»¶çš„å…¨éƒ¨å†…å®¹ï¼Œè¾“å…¥ä¸ºæ–‡ä»¶è·¯å¾„"
    ),
    Tool(
        name="å†™å…¥æ–‡ä»¶",
        func=lambda arg: (
            (lambda filename, content: (open(filename, "w", encoding="utf-8").write(content), "å†™å…¥å®Œæˆ")[1])
        )(*arg.split(",", 1)),
        description="å†™å…¥æ–‡ä»¶å†…å®¹ã€‚è¾“å…¥æ ¼å¼: æ–‡ä»¶å,å†…å®¹ï¼ˆç”¨è‹±æ–‡é€—å·åˆ†éš”ï¼Œå†…å®¹ä¸­è‹¥æœ‰é€—å·è§†ä¸ºæ­£æ–‡çš„ä¸€éƒ¨åˆ†ï¼‰"
    ),
])

print(f"ğŸ”§ å·²åŠ è½½ {len(TOOLS)} ä¸ªå·¥å…·")

EMBED_DIM = 1536  # embedding vector size defined early for stub usage
USE_FAISS = True
try:
    from langchain_community.vectorstores import FAISS
    from langchain_community.docstore.in_memory import InMemoryDocstore
    import faiss
    from langchain_core.documents import Document
    from langchain_core.embeddings import Embeddings as LCEmbeddings
except ImportError as e:
    print(f"âš ï¸ å‘é‡å­˜å‚¨ä¾èµ–ç¼ºå¤±: {e}. ä½¿ç”¨ç®€æ˜“å†…å­˜æ£€ç´¢ä»£æ›¿ FAISSã€‚")
    USE_FAISS = False
    FAISS = None  # type: ignore
    InMemoryDocstore = None  # type: ignore
    faiss = None  # type: ignore
    try:
        from langchain_core.documents import Document  # may still be available
    except Exception:
        class Document:  # minimal fallback
            def __init__(self, page_content: str, metadata: Optional[Dict] = None):
                self.page_content = page_content
                self.metadata = metadata or {}


    class LCEmbeddings:  # type: ignore
        _dim = EMBED_DIM if 'EMBED_DIM' in globals() else 1536

        def embed_query(self, x: str):
            return [0.0] * self._dim

        def embed_documents(self, xs: List[str]):
            return [[0.0] * self._dim for _ in xs]
EMBED_MODEL_NAME = os.getenv("EMBED_MODEL_NAME", "text-embedding-3-small")


class DummyEmbeddings:
    def embed_query(self, t: str):
        return [0.0] * EMBED_DIM

    def embed_documents(self, docs: List[str]):
        return [[0.0] * EMBED_DIM for _ in docs]

    def __call__(self, t: str):
        return self.embed_query(t)


if OPENAI_API_KEY:
    try:
        if IS_DEEPSEEK:
            # DeepSeek çš„ OpenAI å…¼å®¹æ¥å£é€šå¸¸ä¸æä¾› embeddingsï¼›ç›´æ¥ä½¿ç”¨å ä½å‘é‡é¿å… 404
            print("â„¹ï¸ æ£€æµ‹åˆ° DeepSeek base_urlï¼Œè·³è¿‡ OpenAIEmbeddingsï¼Œä½¿ç”¨å ä½åµŒå…¥ä»¥å¯ç”¨è®°å¿†åŠŸèƒ½ã€‚")
            embeddings_model = DummyEmbeddings()
        else:
            from langchain_openai import OpenAIEmbeddings  # lazy import

            embeddings_model = OpenAIEmbeddings(
                model=EMBED_MODEL_NAME,
                api_key=(lambda: OPENAI_API_KEY),
                base_url=OPENAI_BASE_URL
            )
    except Exception as e:
        print(f"âŒ Embeddings åˆå§‹åŒ–å¤±è´¥: {e}; ä½¿ç”¨å ä½ embedding å‡½æ•°ã€‚")
        embeddings_model = DummyEmbeddings()
else:
    embeddings_model = DummyEmbeddings()
    print("âš ï¸ OPENAI_API_KEY ç¼ºå¤±ï¼Œå‘é‡åµŒå…¥ä½¿ç”¨å ä½å‘é‡ã€‚")

_DEF_WORD_RE = re.compile(r"[\u4e00-\u9fff]|[A-Za-z0-9_]+")


def _simple_tokenize(text: str) -> List[str]:
    return _DEF_WORD_RE.findall(text or "")


if USE_FAISS:
    class EmbeddingAdapter(LCEmbeddings):
        """Embeddings adapter implementing LangChain Embeddings interface to silence deprecation warnings."""

        def __init__(self, base):
            self.base = base

        def embed_query(self, x: str) -> List[float]:
            try:
                return self.base.embed_query(x)
            except Exception:
                return [0.0] * EMBED_DIM

        def embed_documents(self, xs: List[str]) -> List[List[float]]:
            try:
                return self.base.embed_documents(xs)
            except Exception:
                return [[0.0] * EMBED_DIM for _ in xs]


    adapter = EmbeddingAdapter(embeddings_model)
    index = faiss.IndexFlatL2(EMBED_DIM)
    vectorstore = FAISS(adapter, index, InMemoryDocstore({}), {})
    print("ğŸ§  å‘é‡æ•°æ®åº“(FAISS)åˆå§‹åŒ–å®Œæˆ")
else:
    class SimpleVectorStore:
        def __init__(self):
            self.docs: List[Document] = []

        def add_documents(self, docs: List[Document]):
            self.docs.extend(docs)

        def similarity_search(self, query: str, k: int = 3) -> List[Document]:
            q_tokens = set(_simple_tokenize(query))

            def score(doc: Document):
                d_tokens = set(_simple_tokenize(doc.page_content))
                if not q_tokens or not d_tokens:
                    return 0.0
                return len(q_tokens & d_tokens) / len(q_tokens | d_tokens)

            ranked = sorted(self.docs, key=score, reverse=True)
            return ranked[:k]


    vectorstore = SimpleVectorStore()
    print("ğŸ§  å‘é‡æ•°æ®ï¿½ï¿½ï¿½ç®€åŒ–ç‰ˆåˆå§‹åŒ–å®Œæˆ(æ— FAISS)")


class MemoryManager:
    def __init__(self, vs, namespace: str = "global"):
        self.vs = vs
        self.namespace = namespace
        self._counter = 0

    def add_memory(self, text: str, metadata: Optional[Dict] = None):
        try:
            meta = metadata or {}
            meta.update({"namespace": self.namespace, "ts": datetime.now().isoformat()})
            doc = Document(page_content=text, metadata=meta)
            self.vs.add_documents([doc])
            self._counter += 1
        except Exception as e:
            print(f"âš ï¸ å†™å…¥è®°å¿†å¤±è´¥: {e}")

    def recall(self, query: str, k: int = 3) -> List[str]:
        try:
            res = self.vs.similarity_search(query, k=k)
            return [d.page_content for d in res]
        except Exception as e:
            print(f"âš ï¸ è¯»å–è®°å¿†å¤±è´¥: {e}")
            return []


from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser


class DualAgentAcademicSystem:
    def __init__(self, llm, tools, vectorstore, enable_tools: bool = True, enable_memory: bool = True,
                 agent_a_llm=None, agent_b_llm=None):
        """Dual-agent academic system.

        Args:
            llm: Default LLM used for both agents when agent_a_llm/agent_b_llm are not provided.
            tools: LangChain Tool list.
            vectorstore: Vector store or SimpleVectorStore.
            enable_tools: Whether to use external tools.
            enable_memory: Whether to use vectorstore-backed long-term memory.
            agent_a_llm: Optional dedicated LLM for Agent A (optimizer), e.g. local Qwen.
            agent_b_llm: Optional dedicated LLM for Agent B (reviewer), e.g. remote DeepSeek.
        """
        self.llm = llm
        self.agent_a_llm = agent_a_llm or llm
        self.agent_b_llm = agent_b_llm or llm
        self.tools_enabled = enable_tools
        self.memory_enabled = enable_memory
        self.tools = {t.name: t for t in tools}
        self.vectorstore = vectorstore
        self.memory = MemoryManager(vectorstore) if enable_memory else None
        self.collaboration_log: List[Dict] = []
        self._setup_agents()

    def _setup_agents(self):
        self.agent_a_template = PromptTemplate.from_template(
            """ä½ æ˜¯Agent A - å­¦æœ¯è¡¨è¾¾ä¼˜åŒ–ä¸“å®¶ã€‚\nè½®æ¬¡: ç¬¬{round_num}è½®\nç”¨æˆ·éœ€æ±‚: {user_requirements}\nä¸Šä¸€è½®è¯„åˆ†(è‹¥æœ‰): {last_scores}\né•¿ç¨‹è®°å¿†æ£€ç´¢ç‰‡æ®µ:\n{memory_snippets}\nå·¥å…·è§‚å¯Ÿ:\n{tool_observations}\nå¾…ä¼˜åŒ–æ–‡æœ¬:\n{text_to_optimize}\n{previous_feedback}\nè¯·è¾“å‡º:\n**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**\n[ä¼˜åŒ–åçš„å®Œæ•´æ–‡æœ¬]\n\n**ä¿®æ”¹è¯´æ˜ï¼š**\n[è¯´æ˜æœ¬è½®ä¿®æ”¹è¦ç‚¹ï¼Œå°¤å…¶é’ˆå¯¹è¯„å®¡æå‡ºçš„é«˜ä¼˜å…ˆçº§é—®é¢˜]"""
        )
        self.agent_b_template = PromptTemplate.from_template(
            """ä½ æ˜¯Agent B - å­¦æœ¯è¯„å®¡ä¸å¯¹æŠ—è´¨è¯¢ä¸“å®¶ã€‚\nè½®æ¬¡: ç¬¬{round_num}è½®\nç”¨æˆ·éœ€æ±‚: {user_requirements}\nä¼˜åŒ–æ–‡æœ¬:\n{optimized_text}\nè¯·è¯„å®¡å¹¶è¾“å‡º(ä¸¥æ ¼åŒ…å«ä»¥ä¸‹æ¿å—ä¸æ•°å€¼)ï¼š\n**æœ¬è½®æ”¹è¿›è¯„ä»·ï¼š**\n[æ€»ä½“è¯„ä»·]\n\n**è¯„åˆ†(è¯·ä½¿ç”¨JSONæ ¼å¼)**\n{{"quality": <1-10>, "rigor": <1-10>, "logic": <1-10>, "novelty": <1-10>, "priority_issues": <æè¿°>}}\n\n**å‰©ä½™ä¸»è¦é—®é¢˜ï¼š**\n[...]\n\n**ä¸‹è½®é‡ç‚¹å»ºè®®ï¼š**\n1. [...]\n2. [...]\n\n**æ”¹è¿›ä¼˜å…ˆçº§ï¼š**\n[é«˜/ä¸­/ä½ åˆ†å±‚åˆ—å‡º]"""
        )
        # use dedicated LLMs for each agent
        self.agent_a_chain = self.agent_a_template | self.agent_a_llm | StrOutputParser()
        self.agent_b_chain = self.agent_b_template | self.agent_b_llm | StrOutputParser()

    @staticmethod
    def _extract_section(text: str, start_token: str, end_token: str) -> str:
        lines = text.split('\n')
        collecting = False
        buf = []
        for l in lines:
            if start_token in l:
                collecting = True
                continue
            if collecting and end_token in l:
                break
            if collecting:
                buf.append(l)
        return '\n'.join(buf).strip()

    def _compute_diff(self, prev: str, current: str) -> str:
        """Unified diff between previous and current text (truncated)."""
        if prev is None:
            return '(é¦–è½®æ— diff)'
        import difflib as _df
        diff_lines = _df.unified_diff(prev.splitlines(), current.splitlines(), lineterm='')
        collected = []
        for i, line in enumerate(diff_lines):
            if i > 400:
                collected.append('... <diff truncated>')
                break
            collected.append(line)
        return '\n'.join(collected) if collected else '(æ— å˜åŒ–)'

    def _parse_scores(self, feedback: str) -> Dict[str, float]:
        import json as _json
        import re as _re
        # ä¿®æ­£æ­£åˆ™ï¼šå»é™¤å†—ä½™çš„è½¬ä¹‰ \}
        # _parse_scores ä¸­åŸæ¨¡å¼ r"\{\s*\"quality\".*?}" -> ä¿ç•™ä¸è½¬ä¹‰çš„ }
        m = _re.search(r"\{\s*\"quality\".*?}", feedback, flags=_re.S)
        if not m:
            return {}
        blob = m.group(0)
        try:
            data = _json.loads(blob)
            for k in ["quality", "rigor", "logic", "novelty"]:
                if k in data:
                    data[k] = float(data[k])
            return data
        except Exception:
            return {}

    def _plan_and_act(self, text: str, requirements: List[str]) -> str:
        if not self.tools_enabled:
            return "(å·¥å…·å·²ç¦ç”¨)"
        observations = []
        joined_req = ' '.join(requirements).lower()
        if any(kw in joined_req for kw in ["search", "æ£€ç´¢", "äº‹å®", "æœ€æ–°", "å¼•ç”¨"]):
            m = re.findall(r'"([^\"]+)"', text)
            query = m[-1] if m else text.split('ã€‚')[-1].strip() or text
            try:
                obs = self.tools["ç½‘ç»œæœç´¢"].run(query)
            except Exception as e:
                obs = f"[æœç´¢å¼‚å¸¸: {e}]"
            observations.append(f"æœç´¢[{query}] -> {obs[:300]}")
        code_blocks = re.findall(r'python:\s*```python\n([\s\S]*?)```', text, flags=re.IGNORECASE)
        for code in code_blocks[:1]:
            try:
                out = self.tools["Python REPL"].run(code)
            except Exception as e:
                out = f"[ä»£ç æ‰§è¡Œå¼‚å¸¸: {e}]"
            observations.append(f"æ‰§è¡ŒPython -> è¾“å‡º: {str(out)[:200]}")
        return "\n".join(observations) if observations else "(æ— )"

    def collaborate(self, user_text: str, user_requirements: List[str], language: str = "ä¸­æ–‡", rounds: int = 3) -> \
    Tuple[str, List[Dict]]:
        self.collaboration_log = [{"round": 0, "user_input": user_text, "requirements": user_requirements,
                                   "timestamp": datetime.now().isoformat()}]
        current_text = user_text
        previous_feedback = ""
        last_scores = {}
        if self.memory_enabled:
            self.memory.add_memory(user_text, {"type": "user_input"})
        for r in range(1, rounds + 1):
            mem_snippets = []
            if self.memory_enabled:
                mem_snippets = self.memory.recall(current_text, k=3)
            tool_obs = self._plan_and_act(current_text, user_requirements)
            a_input = {
                "round_num": r,
                "text_to_optimize": current_text,
                "user_requirements": ', '.join(user_requirements),
                "previous_feedback": previous_feedback,
                "memory_snippets": '\n'.join(mem_snippets) if mem_snippets else "(æ— )",
                "tool_observations": tool_obs,
                "last_scores": last_scores if last_scores else "(æ— )"
            }
            a_resp = self.agent_a_chain.invoke(a_input)
            optimized_text = self._extract_section(a_resp, "**ä¼˜åŒ–ç‰ˆæœ¬ï¼š**", "**ä¿®æ”¹è¯´æ˜ï¼š**") or current_text
            b_input = {
                "round_num": r,
                "optimized_text": optimized_text,
                "user_requirements": ', '.join(user_requirements)
            }
            b_resp = self.agent_b_chain.invoke(b_input)
            last_scores = self._parse_scores(b_resp)
            diff_str = self._compute_diff(current_text, optimized_text)
            if self.memory_enabled:
                self.memory.add_memory(optimized_text, {"type": "optimized_text", "round": r})
                self.memory.add_memory(b_resp, {"type": "feedback", "round": r})
            self.collaboration_log.append({
                "round": r,
                "agent_a_response": a_resp,
                "optimized_text": optimized_text,
                "agent_b_feedback": b_resp,
                "scores": last_scores,
                "tool_observations": tool_obs,
                "diff": diff_str,
                "timestamp": datetime.now().isoformat()
            })
            previous_feedback = b_resp
            current_text = optimized_text
            print(f"âœ… Round {r} å®Œæˆ | è¯„åˆ†: {last_scores if last_scores else '{}'}")
            time.sleep(0.15)
        return current_text, self.collaboration_log

    def synthesize_dataset(self, seeds: List[str], requirements: List[str], rounds: int = 3,
                           out_path: Optional["Path"] = None) -> "Path":
        data_dir = Path("data")
        data_dir.mkdir(parents=True, exist_ok=True)
        if out_path is None:
            out_path = data_dir / f"synth_academic_{datetime.now().strftime('%Y%m%d_%H%M%S')}.jsonl"
        count = 0
        with open(out_path, "w", encoding="utf-8") as f:
            for i, seed in enumerate(seeds):
                final_text, log = self.collaborate(seed, requirements, rounds=rounds)
                record = {
                    "id": f"case_{i}",
                    "input": seed,
                    "requirements": requirements,
                    "final": final_text,
                    "log": log,
                    "created_at": datetime.now().isoformat(),
                    "teacher_signal": log[-1].get("optimized_text", final_text),
                    "scores": log[-1].get("scores", {})
                }
                f.write(json.dumps(record, ensure_ascii=False) + "\n")
                count += 1
        print(f"ğŸ“€ åˆæˆæ•°æ®å·²å†™å…¥: {out_path} (å…± {count} æ¡)")
        return out_path

    @staticmethod
    def _tokenize_zh(text: str) -> List[str]:
        words = re.findall(r"[\u4e00-\u9fff]|[A-Za-z0-9_]+", text)
        return words

    def _readability_proxy(self, text: str) -> float:
        sentences = [s for s in re.split(r'[ã€‚.!?]\s*', text) if s.strip()]
        if not sentences:
            return 0.0
        avg_len = sum(len(s) for s in sentences) / len(sentences)
        # Normalize (heuristic) shorter sentences -> higher readability (invert)
        return round(1 / (1 + avg_len / 50), 4)

    def _coherence_proxy(self, text: str) -> float:
        sentences = [s for s in re.split(r'[ã€‚.!?]\s*', text) if s.strip()]
        if len(sentences) < 2:
            return 0.0

        def tokens(s):
            return set(self._tokenize_zh(s))

        overlaps = []
        for a, b in zip(sentences[:-1], sentences[1:]):
            ta, tb = tokens(a), tokens(b)
            if ta and tb:
                overlaps.append(len(ta & tb) / len(ta | tb))
        return round(sum(overlaps) / len(overlaps), 4) if overlaps else 0.0

    def evaluate(self, cases: List[Tuple[str, List[str]]], rounds: int = 2) -> Dict:
        results = []
        for idx, (text, reqs) in enumerate(cases):
            final_text, log = self.collaborate(text, reqs, rounds=rounds)
            w0 = self._tokenize_zh(text)
            w1 = self._tokenize_zh(final_text)
            len_gain = (len(w1) - len(w0)) / max(1, len(w0))
            ttr0 = len(set(w0)) / max(1, len(w0))
            ttr1 = len(set(w1)) / max(1, len(w1))
            from collections import Counter
            c0 = Counter(w0)
            c1 = Counter(w1)
            rep0 = sum(x for _, x in c0.most_common(5)) / max(1, len(w0))
            rep1 = sum(x for _, x in c1.most_common(5)) / max(1, len(w1))
            readability_gain = self._readability_proxy(final_text) - self._readability_proxy(text)
            coherence_gain = self._coherence_proxy(final_text) - self._coherence_proxy(text)

            def _sentence_lengths(t: str):
                sents = [s for s in re.split(r'[ã€‚.!?]\s*', t) if s.strip()]
                return [len(s) for s in sents] if sents else []

            import statistics
            var0 = statistics.pvariance(_sentence_lengths(text)) if _sentence_lengths(text) else 0.0
            var1 = statistics.pvariance(_sentence_lengths(final_text)) if _sentence_lengths(final_text) else 0.0
            var_delta = round(var0 - var1, 3)

            def _bigram_rep(t: str):
                toks = w1 if t == final_text else w0
                bigrams = [tuple(toks[i:i + 2]) for i in range(len(toks) - 1)]
                bc = Counter(bigrams)
                total = len(bigrams) or 1
                top = sum(v for _, v in bc.most_common(5))
                return top / total

            bigram_delta = round(_bigram_rep(text) - _bigram_rep(final_text), 3)
            last_scores = log[-1].get("scores", {}) if log else {}

            # ============ ä½¿ç”¨æ–°çš„å­¦æœ¯æŒ‡æ ‡ç³»ç»Ÿ ============
            advanced_metrics = {}
            if AcademicMetrics:
                try:
                    original_eval = AcademicMetrics.overall_quality_score(text)
                    optimized_eval = AcademicMetrics.overall_quality_score(final_text)
                    # ç»“æ„å¥å£®æ€§ä¿æŠ¤
                    orig_scores = (original_eval or {}).get('scores', {})
                    opt_scores = (optimized_eval or {}).get('scores', {})
                    comparison = AcademicMetrics.compare_improvements(text, final_text)
                    advanced_metrics = {
                        'original_overall_score': float((original_eval or {}).get('overall_score', 0.0)),
                        'optimized_overall_score': float((optimized_eval or {}).get('overall_score', 0.0)),
                        'academic_formality_improvement': round(
                            float(opt_scores.get('academic_formality', 0.0)) - float(
                                orig_scores.get('academic_formality', 0.0)), 4),
                        'citation_completeness_improvement': round(
                            float(opt_scores.get('citation_completeness', 0.0)) - float(
                                orig_scores.get('citation_completeness', 0.0)), 4),
                        'novelty_improvement': round(
                            float(opt_scores.get('novelty', 0.0)) - float(orig_scores.get('novelty', 0.0)), 4),
                        'language_fluency_improvement': round(float(opt_scores.get('language_fluency', 0.0)) - float(
                            orig_scores.get('language_fluency', 0.0)), 4),
                        'sentence_balance_improvement': round(float(opt_scores.get('sentence_balance', 0.0)) - float(
                            orig_scores.get('sentence_balance', 0.0)), 4),
                        'argumentation_improvement': round(
                            float(opt_scores.get('argumentation', 0.0)) - float(orig_scores.get('argumentation', 0.0)),
                            4),
                        'expression_diversity_improvement': round(
                            float(opt_scores.get('expression_diversity', 0.0)) - float(
                                orig_scores.get('expression_diversity', 0.0)), 4),
                        'structure_completeness_improvement': round(
                            float(opt_scores.get('structure_completeness', 0.0)) - float(
                                orig_scores.get('structure_completeness', 0.0)), 4),
                        'tense_consistency_improvement': round(float(opt_scores.get('tense_consistency', 0.0)) - float(
                            orig_scores.get('tense_consistency', 0.0)), 4),
                    }
                except Exception as e:
                    print(f"âš ï¸ è®¡ç®—é«˜çº§æŒ‡æ ‡å¤±è´¥: {e}")

            results.append({
                "id": idx,
                "len_gain": round(len_gain, 3),
                "ttr_gain": round(ttr1 - ttr0, 3),
                "repetition_delta": round(rep0 - rep1, 3),
                "readability_gain": round(readability_gain, 3),
                "coherence_gain": round(coherence_gain, 3),
                "sent_var_delta": var_delta,
                "bigram_rep_delta": round(bigram_delta, 3),
                "orig_len": len(w0),
                "final_len": len(w1),
                "scores": last_scores,
                "advanced_metrics": advanced_metrics
            })
        # aggregate
        if results:
            avg = {
                "len_gain_avg": round(sum(r["len_gain"] for r in results) / len(results), 3),
                "ttr_gain_avg": round(sum(r["ttr_gain"] for r in results) / len(results), 3),
                "repetition_delta_avg": round(sum(r["repetition_delta"] for r in results) / len(results), 3),
                "readability_gain_avg": round(sum(r["readability_gain"] for r in results) / len(results), 3),
                "coherence_gain_avg": round(sum(r["coherence_gain"] for r in results) / len(results), 3),
                "sent_var_delta_avg": round(sum(r["sent_var_delta"] for r in results) / len(results), 3),
                "bigram_rep_delta_avg": round(sum(r["bigram_rep_delta"] for r in results) / len(results), 3),
                "quality_avg": round(sum(r.get("scores", {}).get("quality", 0) for r in results) / len(results), 3),
                "rigor_avg": round(sum(r.get("scores", {}).get("rigor", 0) for r in results) / len(results), 3),
                "logic_avg": round(sum(r.get("scores", {}).get("logic", 0) for r in results) / len(results), 3),
                "novelty_avg": round(sum(r.get("scores", {}).get("novelty", 0) for r in results) / len(results), 3),
                "n": len(results)
            }

            # æ·»åŠ é«˜çº§æŒ‡æ ‡å¹³å‡å€¼
            if results[0].get("advanced_metrics"):
                adv_avg = {}
                for metric in results[0]["advanced_metrics"].keys():
                    adv_avg[f"{metric}_avg"] = round(
                        sum(r.get("advanced_metrics", {}).get(metric, 0) for r in results) / len(results), 4)
                avg.update(adv_avg)
        else:
            avg = {"len_gain_avg": 0, "ttr_gain_avg": 0, "repetition_delta_avg": 0, "readability_gain_avg": 0,
                   "coherence_gain_avg": 0, "sent_var_delta_avg": 0, "bigram_rep_delta_avg": 0, "quality_avg": 0,
                   "rigor_avg": 0, "logic_avg": 0, "novelty_avg": 0, "n": 0}
        report = {"summary": avg, "cases": results}
        print("ğŸ“ˆ è¯„ä¼°æ±‡æ€»:", json.dumps(report["summary"], ensure_ascii=False))
        return report

    def prepare_distillation_pairs(self, jsonl_path: "Path", out_path: "Path") -> "Path":
        pairs = []
        with open(jsonl_path, 'r', encoding='utf-8') as f:
            for ln in f:
                if not ln.strip():
                    continue
                obj = json.loads(ln)
                instr = f"ä¼˜åŒ–ä»¥ä¸‹å­¦æœ¯æ®µè½ï¼Œæ»¡è¶³éœ€æ±‚: {', '.join(obj.get('requirements', []))}\nåŸæ–‡: {obj.get('input', '')}"
                target = obj.get('teacher_signal', obj.get('final', ''))
                scores = obj.get('scores', {})
                pairs.append({"instruction": instr, "output": target, "scores": scores})
        with open(out_path, 'w', encoding='utf-8') as w:
            for p in pairs:
                w.write(json.dumps(p, ensure_ascii=False) + "\n")
        print(f"ğŸ§ª è’¸é¦æ•°æ®å·²ç”Ÿæˆ: {out_path} å…± {len(pairs)} æ¡")
        return out_path


# Helper to build a hybrid system where Agent A uses a local HF Qwen student model
# and Agent B uses the existing remote teacher llm.
try:
    from hf_student_llm import HFChatLLM as _HFStudentLLM
except Exception:
    _HFStudentLLM = None


def build_hybrid_dual_agent_system(base_model: str | None = None,
                                   lora_dir: str | None = None,
                                   max_new_tokens: int | None = None,
                                   device: str | None = None,
                                   torch_dtype: str | None = None,
                                   device_map: str | None = None) -> DualAgentAcademicSystem:
    """Create a DualAgentAcademicSystem with local HF student model for Agent A and remote LLM for Agent B.
    Added detection of FORCE_STUDENT_STUB=1 æµ‹è¯•/CIæ¨¡å¼.
    """
    if _HFStudentLLM is None:
        print("âš ï¸ HF student LLM not available (transformers/peft missing or import error); using single LLM mode.")
        return DualAgentAcademicSystem(llm, TOOLS, vectorstore)

    # Resolve overrides with env fallback
    env_base = os.getenv("STUDENT_BASE_MODEL", "Qwen/Qwen1.5-1.8B-Chat")
    env_lora = os.getenv("STUDENT_LORA_DIR") or ""
    env_max_new = int(os.getenv("STUDENT_MAX_NEW_TOKENS", "512"))

    base_model = base_model or env_base
    lora_dir = lora_dir if lora_dir is not None else env_lora
    max_new = max_new_tokens or env_max_new

    if lora_dir and not Path(lora_dir).exists():
        print(f"âš ï¸ STUDENT_LORA_DIR not found ({lora_dir}); will load base model without LoRA.")
        lora_dir = ""  # ç»§ç»­ä½†ä¸ä½¿ç”¨LoRA

    stub_mode = os.getenv("FORCE_STUDENT_STUB") == "1"
    if stub_mode:
        print("ğŸ§ª FORCE_STUDENT_STUB=1 å¯ç”¨ï¼šå­¦ç”Ÿæ¨¡å‹ä½¿ç”¨è½»é‡å ä½ç”Ÿæˆï¼Œä¸ä¸‹è½½çœŸå®æƒé‡ã€‚")

    try:
        student_llm = _HFStudentLLM(base_model=base_model,
                                    lora_dir=lora_dir,
                                    max_new_tokens=max_new,
                                    device=device,
                                    torch_dtype=torch_dtype,
                                    device_map=device_map)
        print(f"ğŸ¤– Hybrid mode engaged | Agent A: {getattr(student_llm,'model_name', base_model)} (LoRA={'ON' if lora_dir else 'OFF'} stub={stub_mode}) | Agent B: {getattr(llm, 'model_name', 'remote-llm')}")
        return DualAgentAcademicSystem(llm, TOOLS, vectorstore, agent_a_llm=student_llm, agent_b_llm=llm)
    except Exception as e:
        print(f"âš ï¸ Failed to init student model '{base_model}': {e}; falling back to single LLM mode.")
        return DualAgentAcademicSystem(llm, TOOLS, vectorstore)


# keep default global system for backward compatibility
dual_agent_system = build_hybrid_dual_agent_system()
print("ğŸ§  åŒAgentç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ (Agent A æœ¬åœ°å­¦ç”Ÿæ¨¡å‹, Agent B è¿œç¨‹æ•™å¸ˆæ¨¡å‹, è‹¥å­¦ç”Ÿæœªé…ç½®åˆ™å›é€€å•æ¨¡å‹æ¨¡å¼)")
if os.getenv("FORCE_STUDENT_STUB") == "1":
    print("ğŸ§ª å½“å‰è¿è¡Œäºå­¦ç”Ÿæ¨¡å‹ STUB æ¨¡å¼ï¼šæ‰€æœ‰å­¦ç”Ÿè¾“å‡ºä¸ºå ä½ç”Ÿæˆï¼Œä»…ç”¨äºå¿«é€Ÿæµ‹è¯•ã€‚")

# ä¿®å¤ï¼šè¡¥å…¨å­—ç¬¦ä¸²å¼•å·
ENABLE_INTERACTIVE = os.getenv("ENABLE_INTERACTIVE", "0") == "1"
from html import escape as _html_escape


def generate_html_report(title: str, final_text: str, log: List[Dict], summary: Optional[Dict] = None) -> str:
    if 'round-box' in str(log[:1]):
        return ''

    def style():
        return '''<style>
body{font-family:Segoe UI,Arial,sans-serif;max-width:1200px;margin:32px auto;line-height:1.5}
pre{background:#fafafa;border:1px solid #eee;padding:8px;white-space:pre-wrap;overflow-x:auto}
table{border-collapse:collapse;width:100%;margin:10px 0}
td,th{border:1px solid #ccc;padding:6px 10px;font-size:13px;text-align:left}
th{background:#f0f0f0;font-weight:bold}
.score-badge{display:inline-block;padding:3px 8px;border-radius:4px;background:#004d7a;color:#fff;font-size:12px;margin-right:4px;margin-bottom:4px}
.metric-value{font-weight:bold;color:#2c3e50}
.improvement-positive{color:#27ae60;font-weight:bold}
.improvement-negative{color:#e74c3c;font-weight:bold}
.diff-add{background:#e6ffe6;padding:2px 4px}
.diff-del{background:#ffecec;color:#900;padding:2px 4px}
.round-box{border:1px solid #ddd;padding:12px;margin:12px 0;border-radius:6px;background:#fafafa}
.meta{color:#666;font-size:12px}
.metric-grid{display:grid;grid-template-columns:repeat(auto-fit,minmax(250px,1fr));gap:15px;margin:15px 0}
.metric-card{border:1px solid #e0e0e0;padding:12px;border-radius:6px;background:white}
.metric-card h4{margin:0 0 8px 0;color:#2c3e50;font-size:14px}
.metric-card .value{font-size:20px;font-weight:bold;color:#3498db}
.metric-card .label{font-size:12px;color:#7f8c8d}
.chart-container{margin:15px 0;padding:15px;background:#f9f9f9;border-radius:6px}
details summary{cursor:pointer;font-weight:bold;padding:8px;background:#f0f0f0;margin:8px 0;border-radius:4px}
.progress-bar{width:100%;height:20px;background:#ecf0f1;border-radius:4px;overflow:hidden;margin:5px 0}
.progress-fill{height:100%;background:linear-gradient(to right,#3498db,#2980b9);transition:width 0.3s}
.warning{background:#fff3cd;border-left:4px solid #ffc107;padding:10px;margin:10px 0}
.success{background:#d4edda;border-left:4px solid #28a745;padding:10px;margin:10px 0}
</style>'''

    def render_scores(scores: Dict[str, float]) -> str:
        if not scores:
            return '<span class="meta">æ— è¯„åˆ†</span>'
        return ' '.join(
            f'<span class="score-badge">{k}:{v:.1f}</span>' for k, v in scores.items() if isinstance(v, (int, float)))

    def render_metric_cards(metrics: Dict) -> str:
        """ç”ŸæˆæŒ‡æ ‡å¡ç‰‡ç½‘æ ¼"""
        if not metrics:
            return ''

        html = '<div class="metric-grid">'
        for key, value in metrics.items():
            if key.endswith('_avg') or key.endswith('_improvement'):
                # æå–æ˜“è¯»çš„æ ‡ç­¾å
                label = key.replace('_avg', '').replace('_improvement', '').replace('_', ' ').title()

                # ç¡®å®šé¢œè‰²å’Œæ–¹å‘
                if isinstance(value, (int, float)):
                    if value > 0:
                        color_class = 'improvement-positive'
                        symbol = 'â†‘'
                    elif value < 0:
                        color_class = 'improvement-negative'
                        symbol = 'â†“'
                    else:
                        color_class = ''
                        symbol = '='

                    html += f'''
                    <div class="metric-card">
                        <h4>{label}</h4>
                        <div class="value {color_class}">{symbol} {value:.4f}</div>
                        <div class="label">æ”¹è¿›å¹…åº¦</div>
                    </div>
                    '''
        html += '</div>'
        return html

    def render_advanced_metrics(advanced_metrics: Dict) -> str:
        """æ¸²æŸ“é«˜çº§å­¦æœ¯æŒ‡æ ‡"""
        if not advanced_metrics:
            return ''

        html = '<h3>é«˜çº§å­¦æœ¯æŒ‡æ ‡</h3>'
        html += '<div class="metric-grid">'

        # åŸå§‹å’Œä¼˜åŒ–åçš„æ€»ä½“è¯„åˆ†
        if 'original_overall_score' in advanced_metrics and 'optimized_overall_score' in advanced_metrics:
            orig = advanced_metrics['original_overall_score']
            opt = advanced_metrics['optimized_overall_score']
            improvement = opt - orig

            html += f'''
            <div class="metric-card">
                <h4>æ€»ä½“è´¨é‡è¯„åˆ†</h4>
                <div style="font-size:12px;margin-bottom:8px">
                    ä¼˜åŒ–å‰: <span class="metric-value">{orig:.4f}</span><br/>
                    ä¼˜åŒ–å: <span class="metric-value">{opt:.4f}</span>
                </div>
                <div class="improvement-{('positive' if improvement > 0 else 'negative')}">
                    {improvement:+.4f}
                </div>
            </div>
            '''

        # å„ç»´åº¦æ”¹è¿›
        improvements = {k: v for k, v in advanced_metrics.items() if '_improvement' in k and k != 'overall_improvement'}
        for metric, improvement in sorted(improvements.items())[:6]:  # æ˜¾ç¤ºå‰6ä¸ª
            label = metric.replace('_improvement', '').replace('_', ' ').title()
            color_class = 'improvement-positive' if improvement > 0 else 'improvement-negative'

            # åˆ›å»ºè¿›åº¦æ¡
            progress = max(0, min((improvement + 0.5) / 1.0 * 100, 100))  # æ ‡å‡†åŒ–åˆ°0-100%

            html += f'''
            <div class="metric-card">
                <h4>{label}</h4>
                <div class="progress-bar">
                    <div class="progress-fill" style="width:{progress}%"></div>
                </div>
                <div class="{color_class}">{improvement:+.4f}</div>
            </div>
            '''

        html += '</div>'
        return html

    def color_diff(diff_text: str) -> str:
        lines = []
        for ln in diff_text.splitlines():
            if ln.startswith('+') and not ln.startswith('+++'):
                lines.append(f'<div class="diff-add">{_html_escape(ln)}</div>')
            elif ln.startswith('-') and not ln.startswith('---'):
                lines.append(f'<div class="diff-del">{_html_escape(ln)}</div>')
            else:
                lines.append(f'<div>{_html_escape(ln)}</div>')
        return '\n'.join(lines)

    parts = [f'<html><head><meta charset="utf-8"><title>{_html_escape(title)}</title>{style()}</head><body>']
    parts.append(f'<h1>ğŸ“Š {_html_escape(title)}</h1>')
    parts.append(f'<div class="meta">ç”Ÿæˆæ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}</div>')

    if summary:
        parts.append('<h2>ğŸ“ˆ æŒ‡æ ‡æ±‡æ€»</h2>')

        # æ˜¾ç¤ºåŸºç¡€æŒ‡æ ‡è¡¨æ ¼
        parts.append('<h3>åŸºç¡€æ–‡æœ¬æŒ‡æ ‡</h3>')
        parts.append('<table><tr>')
        basic_metrics = {
            'len_gain_avg': 'é•¿åº¦å˜åŒ–',
            'ttr_gain_avg': 'è¯æ±‡å¤šæ ·æ€§æå‡',
            'repetition_delta_avg': 'é‡å¤åº¦ä¸‹é™',
            'readability_gain_avg': 'å¯è¯»æ€§æå‡',
            'coherence_gain_avg': 'è¿è´¯æ€§æå‡'
        }
        for key in basic_metrics:
            parts.append(f'<th>{basic_metrics[key]}</th>')
        parts.append('</tr><tr>')
        for key in basic_metrics:
            val = summary.get(key, 0)
            color = 'improvement-positive' if val > 0 else 'improvement-negative' if val < 0 else ''
            parts.append(f'<td class="{color}">{val:.4f}</td>')
        parts.append('</tr></table>')

        # æ˜¾ç¤ºAgentè¯„åˆ†
        parts.append('<h3>Agent B è¯„åˆ†æ±‡æ€»</h3>')
        parts.append('<table><tr>')
        agent_metrics = {
            'quality_avg': 'è´¨é‡',
            'rigor_avg': 'ä¸¥è°¨æ€§',
            'logic_avg': 'é€»è¾‘æ€§',
            'novelty_avg': 'æ–°é¢–æ€§'
        }
        for key in agent_metrics:
            parts.append(f'<th>{agent_metrics[key]}</th>')
        parts.append('</tr><tr>')
        for key in agent_metrics:
            val = summary.get(key, 0)
            parts.append(f'<td><span class="metric-value">{val:.2f}/10</span></td>')
        parts.append('</tr></table>')

        # æ˜¾ç¤ºé«˜çº§æŒ‡æ ‡å¡ç‰‡
        adv_metrics = {k: v for k, v in summary.items() if 'improvement' in k}
        if adv_metrics:
            parts.append(render_metric_cards(adv_metrics))

    parts.append('<h2>ğŸ“ æœ€ç»ˆä¼˜åŒ–æ–‡æœ¬</h2><pre>' + _html_escape(final_text) + '</pre>')
    parts.append('<h2>ğŸ“‹ è½®æ¬¡è¯¦ç»†æ—¥å¿—</h2>')
    for entry in log[1:]:
        parts.append('<div class="round-box">')
        parts.append(f'<h3>Round {entry.get("round")}</h3>')
        parts.append(f'<div class="meta">æ—¶é—´: {entry.get("timestamp")}</div>')
        parts.append('<h4>âœï¸ ä¼˜åŒ–æ–‡æœ¬</h4><pre>' + _html_escape(entry.get('optimized_text', '')) + '</pre>')
        parts.append('<h4>ğŸ“ Agent B åé¦ˆ</h4><pre>' + _html_escape(entry.get('agent_b_feedback', '')) + '</pre>')
        parts.append('<h4>â­ è¯„åˆ†</h4>' + render_scores(entry.get('scores', {})))
        parts.append('<details><summary>ğŸ” æ–‡æœ¬å·®å¼‚ (Diff)</summary>' + color_diff(entry.get('diff', '')) + '</details>')
        if entry.get('tool_observations') and entry.get('tool_observations') not in ('(æ— )', '(å·¥å…·å·²ç¦ç”¨)'):
            parts.append('<details><summary>ğŸ”§ å·¥å…·è°ƒç”¨è§‚å¯Ÿ</summary><pre>' + _html_escape(
                entry.get('tool_observations', '')) + '</pre></details>')
        parts.append('</div>')
    parts.append('</body></html>')
    return '\n'.join(parts)


def parse_requirements(raw: Optional[str], fallback: List[str]) -> List[str]:
    if not raw:
        return fallback
    parts = re.split(r"[;,ï¼›]", raw)
    return [p.strip() for p in parts if p.strip()]


def load_seeds_from_file(path: Optional[str]) -> List[str]:
    if not path:
        return []
    p = Path(path)
    if not p.exists():
        print(f"âš ï¸ ç§å­æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        return []
    return [l.strip() for l in p.read_text(encoding='utf-8').splitlines() if l.strip()]


# ---------- Long text splitting & file optimization helpers ----------
def _split_long_text(text: str, chunk_size: int, overlap: int) -> List[str]:
    """Split long text into roughly chunk_size segments with sentence-aware boundaries.
    overlap: number of characters to prepend from previous chunk for continuity (ignored for first chunk)."""
    text = text.strip()
    if chunk_size <= 0:
        return [text]
    # Sentence tokenize (simple)
    sentences = re.split(r'([ã€‚.!?]\s*)', text)  # keep delimiters
    combined = []
    # Reconstruct with delimiters preserved
    buf = ''
    for i in range(0, len(sentences), 2):
        seg = sentences[i]
        delim = sentences[i + 1] if i + 1 < len(sentences) else ''
        piece = seg + delim
        if len(buf) + len(piece) <= chunk_size:
            buf += piece
        else:
            if buf:
                combined.append(buf)
            buf = piece
    if buf:
        combined.append(buf)
    # Apply overlap
    if overlap > 0 and len(combined) > 1:
        with_overlap = []
        prev_tail = ''
        for idx, chunk in enumerate(combined):
            if idx == 0:
                with_overlap.append(chunk)
            else:
                # take tail of previous chunk
                tail = prev_tail[-overlap:] if overlap < len(prev_tail) else prev_tail
                with_overlap.append((tail + chunk).strip())
            prev_tail = chunk
        combined = with_overlap
    return combined


def optimize_text_file(system: DualAgentAcademicSystem, file_path: str, requirements: List[str], rounds: int,
                       chunk_size: int, overlap: int, max_chunks: int = 0) -> Tuple[str, Dict]:
    """Optimize a long text file by chunking and running multi-round collaborate per chunk.
    Returns (final_combined_text, aggregated_report_dict)."""
    p = Path(file_path)
    if not p.exists():
        raise FileNotFoundError(f'æ–‡æœ¬æ–‡ä»¶ä¸å­˜åœ¨: {file_path}')
    raw = p.read_text(encoding='utf-8')
    chunks = _split_long_text(raw, chunk_size, overlap)
    if max_chunks > 0:
        chunks = chunks[:max_chunks]
    segment_logs = []
    optimized_segments = []
    for idx, chunk in enumerate(chunks):
        print(f'ğŸ§© å¤„ç†åˆ†æ®µ {idx + 1}/{len(chunks)} (é•¿åº¦={len(chunk)})')
        final_seg, log = system.collaborate(chunk, requirements, rounds=rounds)
        optimized_segments.append(final_seg)
        segment_logs.append({
            'segment_index': idx,
            'original_length': len(chunk),
            'optimized_length': len(final_seg),
            'final_segment_text': final_seg,
            'round_logs': log
        })
    combined_final = '\n\n'.join(optimized_segments)
    aggregated = {
        'file': file_path,
        'chunks': len(chunks),
        'chunk_size': chunk_size,
        'overlap': overlap,
        'requirements': requirements,
        'final_text': combined_final,
        'segments': segment_logs,
    }
    return combined_final, aggregated


def build_arg_parser():
    p = argparse.ArgumentParser(description='Dual-agent academic optimizer (adversarial enhanced)')
    p.add_argument('command', nargs='?', default='demo', choices=['demo', 'synthesize', 'eval', 'distill'],
                   help='è¿è¡Œæ¨¡å¼')
    p.add_argument('--rounds', type=int, default=2, help='åä½œè½®æ¬¡')
    p.add_argument('--text', type=str, help='è‡ªå®šä¹‰åˆå§‹æ–‡æœ¬ (demo)')
    p.add_argument('--text-file', type=str, help='ä»æ–‡ä»¶è¯»å–åˆå§‹æ–‡æœ¬ (é•¿æ–‡æœ¬ä¼˜åŒ–)')
    p.add_argument('--chunk-size', type=int, default=5000, help='é•¿æ–‡æœ¬åˆ†æ®µå­—ç¬¦æ•° (é»˜è®¤5000, <=0 ä¸åˆ†æ®µ)')
    p.add_argument('--chunk-overlap', type=int, default=200, help='åˆ†æ®µé‡å å­—ç¬¦æ•° (é»˜è®¤200)')
    p.add_argument('--max-chunks', type=int, default=0, help='é™åˆ¶æœ€å¤šå¤„ç†çš„æ®µæ•° (0=ä¸é™åˆ¶)')
    p.add_argument('--requirements', type=str, help='é€—å·/åˆ†å·åˆ†éš”éœ€æ±‚åˆ—è¡¨')
    p.add_argument('--seeds-file', type=str, help='ç§å­æ–‡æœ¬æ–‡ä»¶è·¯å¾„ (synthesize)')
    p.add_argument('--out', type=str, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    p.add_argument('--lang', type=str, choices=['zh', 'en'], default='zh', help='è¯­è¨€')
    p.add_argument('--no-tools', action='store_true', help='ç¦ç”¨å·¥å…·è°ƒç”¨')
    p.add_argument('--no-memory', action='store_true', help='ç¦ç”¨å‘é‡è®°å¿†')
    p.add_argument('--report', type=str, help='JSON æŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    p.add_argument('--html-report', type=str, help='HTML æŠ¥å‘Šè¾“å‡ºè·¯å¾„')
    p.add_argument('--distill-src', type=str, help='è’¸é¦æº JSONL (distill)')
    p.add_argument('--distill-out', type=str, help='è’¸é¦è¾“å‡º JSONL')
    p.add_argument('--out-text-file', type=str, help='å°†æœ€ç»ˆä¼˜åŒ–æ–‡æœ¬å†™å…¥è¯¥è·¯å¾„ (ä¾‹å¦‚ optimized_paper.txt)')
    # HYBRID / Student model parameters
    p.add_argument('--hybrid', action='store_true', help='å¯ç”¨æ··åˆæ¨¡å¼: Agent A ä½¿ç”¨æœ¬åœ°å­¦ç”Ÿæ¨¡å‹, Agent B ä½¿ç”¨è¿œç¨‹æ•™å¸ˆæ¨¡å‹')
    p.add_argument('--student-base-model', type=str, help='å­¦ç”Ÿæ¨¡å‹ HF åç§° (è¦†ç›– STUDENT_BASE_MODEL)')
    p.add_argument('--student-lora-dir', type=str, help='LoRA é€‚é…å™¨ç›®å½• (è¦†ç›– STUDENT_LORA_DIR)')
    p.add_argument('--student-max-new-tokens', type=int, help='å­¦ç”Ÿæ¨¡å‹ç”Ÿæˆæœ€å¤§ new tokens (è¦†ç›– STUDENT_MAX_NEW_TOKENS)')
    p.add_argument('--student-device', type=str, help='å­¦ç”Ÿæ¨¡å‹åŠ è½½è®¾å¤‡ (cpu/cuda è‡ªåŠ¨æ¨æ–­å¯çœç•¥)')
    p.add_argument('--student-dtype', type=str, help='å­¦ç”Ÿæ¨¡å‹ torch dtype, ä¾‹å¦‚ float16/bfloat16')
    p.add_argument('--student-device-map', type=str, help='HF device_map (auto/balanced ç­‰)')
    p.add_argument('--student-save-config', type=str, help='ä¿å­˜å½“å‰å­¦ç”Ÿæ¨¡å‹é…ç½®åˆ°è¯¥ JSON æ–‡ä»¶ (ä¾‹å¦‚ data/student_last_loaded.json)')
    return p


if __name__ == '__main__':
    print('ğŸ“¦ ç¯å¢ƒæ£€æŸ¥: OPENAI_API_KEY={} SERPAPI_API_KEY={}'.format(bool(OPENAI_API_KEY), bool(SERPAPI_API_KEY)))
    parser = build_arg_parser()
    args = parser.parse_args()
    rounds = max(1, args.rounds)

    # åˆå§‹åŒ–ç³»ç»Ÿ: æ ¹æ® --hybrid å†³å®šæ˜¯å¦å¯ç”¨å­¦ç”Ÿæ¨¡å‹
    if args.hybrid:
        dual_agent_system = build_hybrid_dual_agent_system(
            base_model=args.student_base_model,
            lora_dir=args.student_lora_dir,
            max_new_tokens=args.student_max_new_tokens,
            device=args.student_device,
            torch_dtype=args.student_dtype,
            device_map=args.student_device_map,
        )
        # ä¿å­˜é…ç½®ï¼ˆå¯é€‰ï¼‰
        if args.student_save_config:
            cfg_path = Path(args.student_save_config)
            cfg_path.parent.mkdir(parents=True, exist_ok=True)
            config_obj = {
                'base_model': args.student_base_model or os.getenv('STUDENT_BASE_MODEL', 'Qwen/Qwen1.5-1.8B-Chat'),
                'lora_dir': args.student_lora_dir or os.getenv('STUDENT_LORA_DIR') or '',
                'max_new_tokens': args.student_max_new_tokens or int(os.getenv('STUDENT_MAX_NEW_TOKENS', '512')),
                'device': args.student_device,
                'torch_dtype': args.student_dtype,
                'device_map': args.student_device_map,
                'saved_at': datetime.now().isoformat(),
            }
            try:
                cfg_path.write_text(json.dumps(config_obj, ensure_ascii=False, indent=2), encoding='utf-8')
                print(f'ğŸ’¾ å·²ä¿å­˜å­¦ç”Ÿæ¨¡å‹é…ç½®: {cfg_path}')
            except Exception as e:
                print(f'âš ï¸ ä¿å­˜å­¦ç”Ÿæ¨¡å‹é…ç½®å¤±è´¥: {e}')
    else:
        dual_agent_system = DualAgentAcademicSystem(
            llm,
            TOOLS,
            vectorstore,
            enable_tools=not args.no_tools,
            enable_memory=not args.no_memory,
        )
        print('ğŸ” å•æ¨¡å‹æ¨¡å¼: Agent A / B å‡ä½¿ç”¨åŒä¸€è¿œç¨‹æˆ–å›é€€ LLM')

    def _maybe_write_report(data: Dict, path: Optional[str]):
        if path:
            try:
                with open(path, 'w', encoding='utf-8') as f:
                    json.dump(data, f, ensure_ascii=False, indent=2)
                print(f'ğŸ“ JSON æŠ¥å‘Šå·²å†™å…¥: {path}')
            except Exception as e:
                print(f'âš ï¸ JSON æŠ¥å‘Šå†™å…¥å¤±è´¥: {e}')


    def _maybe_write_html(final_text: str, log: List[Dict], path: Optional[str], summary: Optional[Dict] = None,
                          title: str = 'å¤šè½®ä¼˜åŒ–æŠ¥å‘Š'):
        if path:
            try:
                html = generate_html_report(title, final_text, log, summary=summary)
                with open(path, 'w', encoding='utf-8') as f:
                    f.write(html)
                print(f'ğŸ“„ HTML æŠ¥å‘Šå·²å†™å…¥: {path}')
            except Exception as e:
                print(f'âš ï¸ HTML æŠ¥å‘Šå†™å…¥å¤±è´¥: {e}')


    # æ–°å¢ï¼šå°†æœ€ç»ˆä¼˜åŒ–æ–‡æœ¬å†™å…¥åŒç±»å‹æ–‡æœ¬æ–‡ä»¶çš„è¾…åŠ©å‡½æ•°
    def _maybe_write_text(final_text: str, path: Optional[str]):
        if path:
            try:
                out_path = Path(path)
                out_path.parent.mkdir(parents=True, exist_ok=True)
                out_path.write_text(final_text, encoding='utf-8')
                print(f'ğŸ“„ æ–‡æœ¬è¾“å‡ºå·²å†™å…¥: {out_path}')
            except Exception as e:
                print(f'âš ï¸ æ–‡æœ¬è¾“å‡ºå†™å…¥å¤±è´¥: {e}')


    if ENABLE_INTERACTIVE:
        print('ğŸš€ äº¤äº’æ¨¡å¼å¼€å¯')
        if args.text_file:
            # Long file path provided
            reqs = parse_requirements(args.requirements, ['å­¦æœ¯è¡¨è¾¾æå‡'])
            final_text, aggregated = optimize_text_file(dual_agent_system, args.text_file, reqs, rounds=rounds,
                                                        chunk_size=args.chunk_size, overlap=args.chunk_overlap,
                                                        max_chunks=args.max_chunks)
            _maybe_write_report({'final': final_text, 'aggregated': aggregated}, args.report)
            _maybe_write_html(final_text, aggregated.get('segments', []), args.html_report, title='äº¤äº’æ¨¡å¼é•¿æ–‡æœ¬æŠ¥å‘Š')
            _maybe_write_text(final_text, args.out_text_file)
        else:
            final_text, log = dual_agent_system.collaborate(args.text or 'äº¤äº’æ¨¡å¼åˆç¨¿',
                                                            parse_requirements(args.requirements, ['å­¦æœ¯è¡¨è¾¾æå‡']),
                                                            rounds=rounds)
            _maybe_write_report({'final': final_text, 'log': log}, args.report)
            _maybe_write_html(final_text, log, args.html_report, title='äº¤äº’æ¨¡å¼æŠ¥å‘Š')
            _maybe_write_text(final_text, args.out_text_file)
    else:
        if args.command == 'demo':
            print('ğŸš€ Demo æ¼”ç¤ºæ¨¡å¼')
            if args.text_file:
                reqs = parse_requirements(args.requirements,
                                          ['å­¦æœ¯è¡¨è¾¾æå‡', 'é€»è¾‘ç»“æ„ä¼˜åŒ–'] if args.lang == 'zh' else ['academic polish',
                                                                                                      'logical coherence'])
                final_text, aggregated = optimize_text_file(dual_agent_system, args.text_file, reqs, rounds=rounds,
                                                            chunk_size=args.chunk_size, overlap=args.chunk_overlap,
                                                            max_chunks=args.max_chunks)
                print('\nğŸ“Œ Long file optimized final text (truncated preview):\n',
                      final_text[:800] + ('...' if len(final_text) > 800 else ''))
                _maybe_write_report({'final': final_text, 'aggregated': aggregated}, args.report)
                _maybe_write_html(final_text, aggregated.get('segments', []), args.html_report, title='é•¿æ–‡æœ¬ä¼˜åŒ–æŠ¥å‘Š')
                _maybe_write_text(final_text, args.out_text_file)
            else:
                base_default = (
                    'This is a preliminary draft about multi-agent collaboration in academic writing.' if args.lang == 'en' else 'è¿™æ˜¯ä¸€æ®µå…³äºå¤šæ™ºèƒ½ä½“åä½œè¿›è¡Œå­¦æœ¯å†™ä½œä¼˜åŒ–çš„åˆç¨¿ã€‚'
                )
                sample_text = args.text or base_default
                reqs = parse_requirements(args.requirements,
                                          ['å­¦æœ¯è¡¨è¾¾æå‡', 'é€»è¾‘ç»“æ„ä¼˜åŒ–'] if args.lang == 'zh' else ['academic polish',
                                                                                                      'logical coherence'])
                final_text, log = dual_agent_system.collaborate(sample_text, reqs, rounds=rounds)
                print('\nğŸ“Œ Final optimized text:\n', final_text)
                _maybe_write_report({'final': final_text, 'log': log}, args.report)
                _maybe_write_html(final_text, log, args.html_report, title='Demo ä¼˜åŒ–æŠ¥å‘Š')
                _maybe_write_text(final_text, args.out_text_file)
        elif args.command == 'synthesize':
            print('ğŸ§ª æ•°æ®åˆæˆæ¨¡å¼')
            seeds = load_seeds_from_file(args.seeds_file) or [
                'æœ¬ç ”ç©¶æ¢è®¨äº†åŸºäºå¤šæ™ºèƒ½ä½“çš„æ–‡æœ¬ä¼˜åŒ–æ¡†æ¶ï¼Œåˆæ­¥å®éªŒå°šä¸å……åˆ†ã€‚',
                'æˆ‘ä»¬æå‡ºä¸€ä¸ªç®€å•çš„ç®¡çº¿ï¼Œä½†æ–¹æ³•éƒ¨åˆ†ç¼ºä¹æ¸…æ™°çš„å› æœè®ºè¯ã€‚',
                'å®éªŒç»“æœæ˜¾ç¤ºä¸€å®šæ”¹è¿›ï¼Œä½†ç»Ÿè®¡æ˜¾è‘—æ€§éœ€è¦è¿›ä¸€æ­¥è¯´æ˜ã€‚',
            ]
            reqs = parse_requirements(args.requirements, ['å­¦æœ¯è¡¨è¾¾æå‡', 'ç»“æ„æ¸…æ™°', 'å¯è¯»æ€§å¢å¼º'])
            out_path = Path(args.out) if args.out else None
            path = dual_agent_system.synthesize_dataset(seeds, reqs, rounds=rounds, out_path=out_path)
            _maybe_write_report({'dataset_path': str(path)}, args.report)
            if args.html_report:
                print('â„¹ï¸ synthesize æ¨¡å¼ä¸ç”Ÿæˆå•ä¸€æµç¨‹ HTML æŠ¥å‘Šï¼Œå¿½ç•¥ --html-report')
        elif args.command == 'eval':
            print('ğŸ§® è¯„ä¼°æ¨¡å¼')
            tests = [
                ('æœ¬æ–‡æå‡ºä¸€ç§æ–¹æ³•ï¼Œä½†å­˜åœ¨ä¸€äº›é—®é¢˜ï¼Œéœ€è¦æ›´ä¸¥è°¨çš„å™è¿°ã€‚',
                 parse_requirements(args.requirements, ['ä¸¥è°¨æ€§', 'é€»è¾‘è¿è´¯'])),
                ('æˆ‘ä»¬çš„å®éªŒç»“æœè¾ƒä¸ºæœ‰é™ï¼Œç¼ºå°‘æ¶ˆèå®éªŒã€‚',
                 parse_requirements(args.requirements, ['è¡¥å……å®éªŒå»ºè®®', 'å­¦æœ¯åŒ–è¡¨è¾¾']))
            ]
            report = dual_agent_system.evaluate(tests, rounds=rounds)
            _maybe_write_report(report, args.report)
            _maybe_write_html('N/A (Eval å¤šæ¡ˆä¾‹)', report.get('cases', []), args.html_report,
                              summary=report.get('summary'), title='è¯„ä¼°æŒ‡æ ‡æ±‡æ€»æŠ¥å‘Š')
        elif args.command == 'distill':
            print('ğŸ§ª è’¸é¦æ•°æ®ç”Ÿæˆæ¨¡å¼')
            src = Path(args.distill_src) if args.distill_src else Path(args.out or 'data/latest_synth.jsonl')
            if not src.exists():
                print(f'âš ï¸ è’¸é¦æºä¸å­˜åœ¨: {src}')
            else:
                distill_out = Path(args.distill_out) if args.distill_out else Path('data/distill_pairs.jsonl')
                distill_out.parent.mkdir(parents=True, exist_ok=True)
                dual_agent_system.prepare_distillation_pairs(src, distill_out)
                _maybe_write_report({'distill_pairs': str(distill_out)}, args.report)
                if args.html_report:
                    print('â„¹ï¸ distill æ¨¡å¼ä¸ç”Ÿæˆ HTML æŠ¥å‘Šï¼Œå¿½ç•¥ --html-report')
